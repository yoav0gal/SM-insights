{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET UP\n",
    "1). create a conda env called BERTopic python 3.12.8.\n",
    "\n",
    "2). Connect the jupiter notebook to that env (kernel).\n",
    "\n",
    "3). install dependencies \n",
    "\n",
    "4). Create your dataset (if you don't want to use the existing one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture #remove the capture prefix/use your terminal for debug output \n",
    "!conda create --name BERTopic python=3.12.8 -y\n",
    "!conda activate BERTopic # powershell gave problems and cmd worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture #remove the capture prefix/use your terminal for debug output \n",
    "!conda install -n BERTopic -c conda-forge --file Notebooks/requirements.txt -y\n",
    "\n",
    "# Option 2 Mamba\n",
    "# conda activate BERTopic\n",
    "# conda install -c conda-forge mamba -y\n",
    "# mamba install --file Notebooks/requirements.txt -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new dataset (optional)\n",
    "**note**: this is an optional step, we recommend using an existing dataset. \n",
    "\n",
    "1). Run the `sm-insights-next` project locally (cd to sm-insights-next && npm run dev:https) \n",
    "\n",
    "2). Run the `create_youtube_dataset` with your params.\n",
    "\n",
    "3). If all good the dataset would be saved under `/Notebooks/datasets/youtube-comments/filename.cvs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def create_youtube_dataset( video_id=\"gXjj2EoElFg\", dataset_name=\"JackVsCalley\", limit=500):\n",
    "    base_url=\"https://localhost:3000\"\n",
    "    endpoint = f\"{base_url}/api/youtube/comments/create-dataset\"\n",
    "    \n",
    "    params = {\n",
    "        \"video_id\": video_id,\n",
    "        \"data_set_name\": dataset_name,\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Disable SSL verification for localhost\n",
    "        response = requests.get(endpoint, params=params, verify=False)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName=\"honey_scam_500\"\n",
    "video_id=\"vc4yL3YTwWk\" #the value of the v url parameter on a youtube video link on your desktop\n",
    "limit=500\n",
    "\n",
    "# response  = create_youtube_dataset(video_id=video_id,dataset_name=datasetName,limit=limit)\n",
    "# response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName = \"jack_vs_calley_1000\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and run BERTtopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Load the comments dataset\n",
    "df = pd.read_csv(f\"../datasets/youtube-comments/{datasetName}.csv\") \n",
    "\n",
    "# Assuming your CSV has a column named 'text' containing the comments\n",
    "comments = df['text'].tolist()\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2') \n",
    "# Create and fit the BERTopic model\n",
    "model = BERTopic( \n",
    "    vectorizer_model=vectorizer_model,\n",
    "    embedding_model=sentence_model,\n",
    "    language='english',\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True)\n",
    "\n",
    "topics, probabilities = model.fit_transform(comments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of cluster labeling using keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "topic_labels = {}\n",
    "for topic in range(len(set(topics))-1):\n",
    "    words = model.get_topic(topic)\n",
    "    keywords = kw_model.extract_keywords(' '.join([word[0] for word in words]), keyphrase_ngram_range=(1, 2), top_n=1) \n",
    "    topic_labels[topic] = keywords[0][0]\n",
    "\n",
    "\n",
    "\n",
    "model.set_topic_labels(topic_labels=topic_labels)\n",
    "topic_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments and their label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f'{topic_labels[i]}: {comments[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = model.get_topic_info()\n",
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualizations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics(custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_hierarchy(custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_barchart(custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_heatmap(custom_labels=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERTopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
