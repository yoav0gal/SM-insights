{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic based clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our project is to group YouTube comments together based on their topics.\n",
    "\n",
    "This notebook will explore different clustering algorithms, benchmark them, and recommend one that is most suitable for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem and the goal\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "We need an automated way to group YouTube video comments by their underlying topics based on text embeddings.\n",
    "\n",
    "Input challageds:\n",
    "- dynamic\n",
    "- no pre-defined labels\n",
    "- unknown number of natural topics (`k`)\n",
    "- requires fast processing for a user-facing application.\n",
    "\n",
    "**The Goal:**\n",
    "\n",
    "This notebook explores and implements clustering strategies that can rapidly process comment embeddings \"on the fly\". The aim is to find methods that quickly identify meaningful topic clusters for any video. \n",
    "\n",
    "Optimizing for internal validation scores:\n",
    " - Silhouette\n",
    " - Davies-Bouldin\n",
    " - Calinski-Harabasz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes the following: \n",
    "- Comments have already been cleaned and encoded. \n",
    "- Comments encoding have been reduced into a 2 dimensional space.\n",
    "\n",
    "Currently, we did not prefect these steps, so here is a short and imperfect implementation of these so we can start working. [link to notebook](./assumptions.ipynb)\n",
    "\n",
    "> We assume that the data is encoded properly and that by mesuring the distance between comments, we can cluster them based on topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "- Agglomerative\n",
    "- K-Means\n",
    "- HDBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Benchmarks\n",
    "- Davies-Bouldin\n",
    "- Silhouette\n",
    "- Calinski-Harabasz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install scikit-learn pandas numpy tqdm python-dotenv google-genai plotly matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering algorithms can be grouped into 4 main categories.\n",
    "\n",
    "![Clustering Algorithm Categories](./images/clustering%20groups.png)\n",
    "\n",
    "1.  **Flat vs. Hierarchical Output:** Does the algorithm produce a single partition or a nested structure?\n",
    "2.  **Core Mechanism:** Is it based on distance to centers (**Centroid/Parametric**) or on data point density (**Density/Non-Parametric**)?\n",
    "\n",
    "Understanding the strengths of these categories helps guide algorithm selection:\n",
    "\n",
    "* **Flat & Centroid/Parametric (e.g., KMeans, GMM):**\n",
    "    * **Strength:** Efficient partitioning into a predefined number (`k`) of typically globular/elliptical clusters. Good for large datasets when `k` is known and shapes are simple.\n",
    "* **Hierarchical & Centroid/Parametric (e.g., Ward, Agglomerative Linkages):**\n",
    "    * **Strength:** Reveals nested cluster structures (dendrogram), useful for exploring different granularities without fixing `k` initially. Ward linkage is good for finding compact, balanced clusters within the hierarchy.\n",
    "* **Flat & Density/Non-Parametric (e.g., DBSCAN, Mean Shift):**\n",
    "    * **Strength:** Discovering arbitrarily shaped clusters and identifying noise without needing `k` upfront. Effective when density defines groups better than proximity to a center.\n",
    "* **Hierarchical (Internal) & Density/Non-Parametric (e.g., HDBSCAN):**\n",
    "    * **Strength:** Robustly finds arbitrarily shaped clusters of varying densities, handles noise, and automatically determines a suitable number of clusters (`k`) based on stability. Excellent for exploration when `k` and shapes are unknown. (Note: Outputs a flat partition).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(n_samples=500, random_state=76):\n",
    "    datasets_dict = {}\n",
    "\n",
    "    X_blobs, y_blobs = datasets.make_blobs(\n",
    "        n_samples=n_samples, centers=3, cluster_std=0.8, random_state=random_state\n",
    "    )\n",
    "    datasets_dict['Blobs'] = (X_blobs, y_blobs)\n",
    "\n",
    "    X_moons, y_moons = datasets.make_moons(\n",
    "        n_samples=n_samples, noise=0.05, random_state=random_state\n",
    "    )\n",
    "    datasets_dict['Moons'] = (X_moons, y_moons)\n",
    "\n",
    "    X_circles, y_circles = datasets.make_circles(\n",
    "        n_samples=n_samples, factor=0.5, noise=0.05, random_state=random_state\n",
    "    )\n",
    "    datasets_dict['Circles'] = (X_circles, y_circles)\n",
    "\n",
    "    X_varied, y_varied = datasets.make_blobs(\n",
    "        n_samples=n_samples,\n",
    "        centers=[[1, 1], [-2, -2], [3, -1]],\n",
    "        cluster_std=[0.5, 1.5, 0.3],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    datasets_dict['Varied Blobs'] = (X_varied, y_varied)\n",
    "\n",
    "    X_aniso, y_aniso = datasets.make_blobs(\n",
    "        n_samples=n_samples, centers=3, random_state=random_state\n",
    "    )\n",
    "    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "    X_aniso = np.dot(X_aniso, transformation)\n",
    "    datasets_dict['Anisotropic Blobs'] = (X_aniso, y_aniso)\n",
    "\n",
    "    X_noise = np.random.rand(n_samples, 2) * 10\n",
    "    datasets_dict['No Structure (Noise)'] = (X_noise, None)\n",
    "\n",
    "    print(f\"Generated datasets: {list(datasets_dict.keys())}\")\n",
    "    return datasets_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating sample datasets...\")\n",
    "sample_datasets = generate_datasets(n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, labels, title=\"Clustering Results\", noise_color='lightgrey', palette='viridis', noise_marker='x', figsize=(8, 6)):\n",
    "    if X.shape[1] != 2:\n",
    "        print(\"Warning: Plotting function currently only supports 2D data.\")\n",
    "        return\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    if n_clusters > 0:\n",
    "        colors = sns.color_palette(palette, n_colors=n_clusters)\n",
    "        label_color_map = {label: colors[i] for i, label in enumerate(sorted(l for l in unique_labels if l != -1))}\n",
    "    else:\n",
    "        label_color_map = {}\n",
    "\n",
    "    label_color_map[-1] = noise_color\n",
    "\n",
    "    is_noise = (labels == -1)\n",
    "    if np.any(is_noise):\n",
    "        plt.scatter(X[is_noise, 0], X[is_noise, 1], c=label_color_map[-1], marker=noise_marker, label='Noise', alpha=0.5, s=30, zorder=1)\n",
    "\n",
    "    for label, color in label_color_map.items():\n",
    "        if label == -1: continue\n",
    "        cluster_points = X[labels == label]\n",
    "        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], color=color, label=f'Cluster {label}', alpha=0.8, s=50, zorder=2)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    handles, current_labels = plt.gca().get_legend_handles_labels()\n",
    "    if handles:\n",
    "      plt.legend(scatterpoints=1, loc='best', ncol=1, fontsize=8)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_datasets(datasets_dict, model, model_name, grid_shape=(2, 3), figsize=(12, 9), point_size=10, cmap='viridis'):\n",
    "    rows, cols = grid_shape\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    dataset_items = list(datasets_dict.items())\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < len(dataset_items):\n",
    "            name, (X, y_true) = dataset_items[i]\n",
    "\n",
    "            try:\n",
    "                model.fit(X)\n",
    "                y_pred = model.labels_\n",
    "            except Exception as e:\n",
    "                print(f\"Error fitting model to dataset '{name}': {e}\")\n",
    "                y_pred = None\n",
    "\n",
    "            if y_pred is not None:\n",
    "                unique_labels = set(y_pred)\n",
    "                if -1 in unique_labels:\n",
    "                    noise_mask = (y_pred == -1)\n",
    "                    scatter = ax.scatter(X[~noise_mask, 0], X[~noise_mask, 1], c=y_pred[~noise_mask], \n",
    "                                        cmap='viridis', s=point_size, alpha=0.7)\n",
    "                    ax.scatter(X[noise_mask, 0], X[noise_mask, 1], c='gray', s=point_size, alpha=0.5)\n",
    "                else:\n",
    "                    scatter = ax.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=point_size, alpha=0.7)\n",
    "            else:\n",
    "                scatter = ax.scatter(X[:, 0], X[:, 1], s=point_size, alpha=0.7, color='gray')\n",
    "\n",
    "            ax.set_title(f\"{name}\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            # Set limits to exactly match the data range\n",
    "            x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "            y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "            \n",
    "            # Add minimal padding (1%) to prevent points from touching the edge\n",
    "            x_padding = 0.01 * (x_max - x_min)\n",
    "            y_padding = 0.01 * (y_max - y_min)\n",
    "            \n",
    "            ax.set_xlim(x_min - x_padding, x_max + x_padding)\n",
    "            ax.set_ylim(y_min - y_padding, y_max + y_padding)\n",
    "            ax.set_aspect('equal')\n",
    "            \n",
    "            # Keep borders visible\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(True)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Clustering Results using {model_name}\", fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_datasets(sample_datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring HDBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why HDBSCAN is a Good Fit Here\n",
    "\n",
    "HDBSCAN stands out as a strong candidate for clustering YouTube comment embeddings for several key reasons:\n",
    "\n",
    "* **Finds Number of Clusters (`k`) Automatically:** Unlike KMeans, it figures out the most stable number of clusters based on the data's density, which is essential when `k` is unknown and varies between videos.\n",
    "* **Handles Complex Clusters:** As a density-based method, it can find clusters of various shapes and densities, which is useful as topic clusters in embedding space might not be simple spheres.\n",
    "* **Identifies Noise:** It explicitly labels outlier or irrelevant comments as noise (-1), separating them from the core topic clusters.\n",
    "* **Robust & Flexible:** It generally requires less sensitive parameter tuning than similar methods like DBSCAN and fits well when we don't want to assume simple cluster shapes or a fixed `k`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How HDBSCAN Works (Conceptual Overview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "HDBSCAN refines density-based clustering by creating a cluster hierarchy and selecting the most *stable* groups. Here’s the essence:\n",
    "\n",
    "1.  **Adjust Distances based on Density:**\n",
    "    * Calculates **core distance** for each point `x` (distance to its k-th neighbor, reflecting local density):\n",
    "        $$ \\text{core}_k(x) = d(x, \\text{k-th NN of } x) $$\n",
    "    * Computes **mutual reachability distance** between points `a` and `b`, using core distances to effectively shorten distances within dense regions compared to sparse ones:\n",
    "        $$ d_{\\text{mreach}-k}(a, b) = \\max(\\text{core}_k(a), \\text{core}_k(b), d(a, b)) $$\n",
    "\n",
    "![DBSCAN gif 1](./images/DBSCAN_GIF_1.gif) ![DBSCAN gif 2](./images/DBSCAN_gif_2.gif)\n",
    "\n",
    "\n",
    "2.  **Build Hierarchy from Dense Connections:**\n",
    "    * Constructs a Minimum Spanning Tree (MST) using mutual reachability distances, connecting points primarily through dense areas.\n",
    "    * Creates a hierarchy of potential clusters by simulating splits in the MST as the distance threshold increases (like varying `epsilon` in DBSCAN*).\n",
    "\n",
    "    ![hiracical gif](./images/hierarch_gif_1.gif)\n",
    "\n",
    "3.  **Select Stable Clusters:**\n",
    "    * Prunes the hierarchy based on `min_cluster_size` (removing tiny, insignificant splits).\n",
    "    * **Key Step:** Measures cluster **stability** – essentially, how long a cluster persists across a range of density levels ($\\lambda = 1/\\text{distance}$). Stability can be approximated by summing individual point persistence within the cluster:\n",
    "        $$ \\text{Stability}(C) \\approx \\sum_{p \\in C} (\\lambda_{p, \\text{leaves}} - \\lambda_{C, \\text{birth}}) $$\n",
    "    * It extracts the clusters that maximize this stability metric. Points not part of any selected stable cluster are designated as noise (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "hdbscan_model_instance = HDBSCAN(\n",
    "    min_cluster_size=15,    \n",
    "    metric='euclidean',      \n",
    ")\n",
    "\n",
    "hdbscan_model_name = \"HDBSCAN (min_cluster_size=15)\"\n",
    "\n",
    "\n",
    "plot_all_datasets(\n",
    "    datasets_dict=sample_datasets,\n",
    "    model=hdbscan_model_instance,\n",
    "    model_name=hdbscan_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering preformance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we explore how to evaluate clustering results when we have no true labels. We generate a synthetic dataset, visualize it using Plotly, then cluster it in different ways. The key question is: **How will we know which clustering is the best?**  \n",
    "We answer that by comparing evaluation metrics:\n",
    "- **Silhouette Score**: Measures how similar a point is to its own cluster versus other clusters (range: -1 to 1; higher is better).\n",
    "- **Davies-Bouldin Index**: Quantifies the average similarity between each cluster and its most similar one (lower is better).\n",
    "- **Calinski-Harabasz Index**: Computes the ratio of between-cluster dispersion to within-cluster dispersion (higher is better).\n",
    "\n",
    "We also show how to optimize the clustering (i.e., choosing the number of clusters) using these metrics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Setup and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data(n_samples=400, centers=5, random_state=20):\n",
    "    \"\"\"Generate synthetic data using make_blobs.\"\"\"\n",
    "    X, _ = make_blobs(n_samples=n_samples, centers=centers, random_state=random_state)\n",
    "    return X\n",
    "\n",
    "def plot_data(X, labels=None, title=\"Data Visualization\"):\n",
    "    \"\"\"Plot data using Plotly.\"\"\"\n",
    "    if labels is None:\n",
    "        fig = px.scatter(x=X[:, 0], y=X[:, 1], title=title)\n",
    "    else:\n",
    "        fig = px.scatter(x=X[:, 0], y=X[:, 1], color=labels.astype(str), title=title)\n",
    "    fig.show()\n",
    "\n",
    "X = generate_data()\n",
    "plot_data(X, title=\"Original Data\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cluster_data(X, n_clusters):\n",
    "    \"\"\"Cluster data using KMeans.\"\"\"\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = model.fit_predict(X)\n",
    "    return labels\n",
    "\n",
    "def cluster_agglomerative(X, n_clusters):\n",
    "    model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    return model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_fixed = kmeans_cluster_data(X, n_clusters=3)\n",
    "plot_data(X, labels_fixed, title=\"k-means Clustering with 3 Clusters\")\n",
    "\n",
    "labels_fixed = kmeans_cluster_data(X, n_clusters=5)\n",
    "plot_data(X, labels_fixed, title=\"k-means Clustering with 5 Clusters\")\n",
    "\n",
    "labels_fixed = kmeans_cluster_data(X, n_clusters=10)\n",
    "plot_data(X, labels_fixed, title=\"k-means Clustering with 10 Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_fixed = cluster_agglomerative(X, n_clusters=3)\n",
    "plot_data(X, labels_fixed, title=\"agglomerative Clustering with 3 Clusters\")\n",
    "\n",
    "labels_fixed = cluster_agglomerative(X, n_clusters=5)\n",
    "plot_data(X, labels_fixed, title=\"agglomerative Clustering with 5 Clusters\")\n",
    "\n",
    "labels_fixed = cluster_agglomerative(X, n_clusters=10)\n",
    "plot_data(X, labels_fixed, title=\"agglomerative Clustering with 10 Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize_optimization_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "def visualize_optimization_results(X, cluster_range, scores, cluster_func, algorithm_name, score_func, score_name, maximize=True):\n",
    "    \"\"\"\n",
    "    Visualizes clustering optimization results for any evaluation metric.\n",
    "\n",
    "    It plots:\n",
    "    - The evaluation score versus the number of clusters.\n",
    "    - Clustering visualizations for three key cases:\n",
    "      * Initial (first value in cluster_range)\n",
    "      * Middle (middle value in cluster_range)\n",
    "      * Best (based on the provided evaluation metric)\n",
    "    \n",
    "    Parameters:\n",
    "        X : np.array\n",
    "            The data to be clustered.\n",
    "        cluster_range : list or range\n",
    "            A sequence of cluster counts that were evaluated.\n",
    "        scores : list of float\n",
    "            The evaluation scores corresponding to each cluster count.\n",
    "        cluster_func : function\n",
    "            A clustering function that accepts (X, n_clusters) and returns cluster labels.\n",
    "        algorithm_name : str\n",
    "            The name of the clustering algorithm (for labeling plots).\n",
    "        score_func : function\n",
    "            A function that calculates the evaluation score given (X, labels).\n",
    "        score_name : str\n",
    "            The name of the evaluation metric (e.g., \"Silhouette Score\").\n",
    "        maximize : bool (default=True)\n",
    "            Set True if a higher score is better (e.g., Silhouette Score), or False if lower is better (e.g., Davies-Bouldin Index).\n",
    "    \"\"\"\n",
    "    # Convert cluster_range to a list (if not already)\n",
    "    cluster_range = list(cluster_range)\n",
    "    \n",
    "    # Determine key points: initial, middle, and best cluster counts.\n",
    "    initial_k = cluster_range[0]\n",
    "    middle_index = len(cluster_range) // 2\n",
    "    middle_k = cluster_range[middle_index]\n",
    "    best_index = np.argmax(scores) if maximize else np.argmin(scores)\n",
    "    best_k = cluster_range[best_index]\n",
    "    \n",
    "    # Print details\n",
    "    print(f\"{algorithm_name} Optimization for {score_name}:\")\n",
    "    print(f\"Initial: {initial_k} clusters, {score_name}: {scores[0]:.3f}\")\n",
    "    print(f\"Middle: {middle_k} clusters, {score_name}: {scores[middle_index]:.3f}\")\n",
    "    print(f\"Best: {best_k} clusters, {score_name}: {scores[best_index]:.3f}\")\n",
    "    \n",
    "    # Plot the optimization graph using matplotlib.\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(cluster_range, scores, marker='o', label=algorithm_name)\n",
    "    plt.scatter([initial_k, middle_k, best_k],\n",
    "                [scores[0], scores[middle_index], scores[best_index]],\n",
    "                color='red', zorder=5, label='Selected Points')\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(score_name)\n",
    "    plt.title(f\"{algorithm_name} Optimization Graph ({score_name})\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize clustering results for the initial, middle, and best cases using Plotly.\n",
    "    for k, label in zip([initial_k, middle_k, best_k], [\"Initial\", \"Middle\", \"Best\"]):\n",
    "        cluster_labels = cluster_func(X, k)\n",
    "        current_score = score_func(X, cluster_labels)\n",
    "        fig = px.scatter(x=X[:, 0], y=X[:, 1], color=cluster_labels.astype(str),\n",
    "                         title=f\"{algorithm_name} {label} Clustering: {k} Clusters ({score_name} = {current_score:.3f})\")\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know wich clustring algorithm is the best? what are the best hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Silhouette Score** measures how similar a data point is to its own cluster compared to other clusters. For each sample \\(i\\), the silhouette score is given by:\n",
    "\n",
    "$$\n",
    "s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\(a(i)\\) is the average distance between \\(i\\) and all other points in the same cluster,  \n",
    "- \\(b(i)\\) is the minimum average distance from \\(i\\) to points in a different cluster (i.e., the closest cluster that \\(i\\) is not a member of).\n",
    "\n",
    "A higher Silhouette Score (close to 1) indicates that the sample is well matched to its own cluster and poorly matched to neighboring clusters. We use this metric to optimize the clustering (by tuning the number of clusters) and to compare different clustering algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "cluster_range = range(2, 10)\n",
    "\n",
    "scores_kmeans = []\n",
    "for k in cluster_range:\n",
    "    labels = kmeans_cluster_data(X, k)\n",
    "    score = silhouette_score(X, labels)\n",
    "    scores_kmeans.append(score)\n",
    "\n",
    "visualize_optimization_results(\n",
    "    X, \n",
    "    cluster_range, \n",
    "    scores_kmeans, \n",
    "    kmeans_cluster_data, \n",
    "    algorithm_name=\"KMeans\", \n",
    "    score_func=silhouette_score, \n",
    "    score_name=\"Silhouette Score\", \n",
    "    maximize=True\n",
    ")\n",
    "\n",
    "scores_agg = []\n",
    "for k in cluster_range:\n",
    "    labels = cluster_agglomerative(X, k)\n",
    "    score = silhouette_score(X, labels)\n",
    "    scores_agg.append(score)\n",
    "\n",
    "visualize_optimization_results(\n",
    "    X, \n",
    "    cluster_range, \n",
    "    scores_agg, \n",
    "    cluster_agglomerative, \n",
    "    algorithm_name=\"Agglomerative Clustering\", \n",
    "    score_func=silhouette_score, \n",
    "    score_name=\"Silhouette Score\", \n",
    "    maximize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Bouldin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Davies-Bouldin Index (DB)** measures the average similarity between each cluster and its most similar (i.e., most overlapping) cluster. It is defined as:\n",
    "\n",
    "$$\n",
    "DB = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\neq i} \\left(\\frac{s_i + s_j}{d(c_i, c_j)}\\right)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\(k\\) is the number of clusters,  \n",
    "- \\(s_i\\) is the average distance between each point in cluster \\(i\\) and the centroid of cluster \\(i\\) (a measure of cluster compactness),  \n",
    "- \\(d(c_i, c_j)\\) is the distance between the centroids of clusters \\(i\\) and \\(j\\).\n",
    "\n",
    "**Interpretation:**  \n",
    "A lower Davies-Bouldin Index indicates better clustering performance, as it suggests that clusters are compact and well separated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "cluster_range = range(2, 10)\n",
    "\n",
    "scores_kmeans = []\n",
    "for k in cluster_range:\n",
    "    labels = kmeans_cluster_data(X, k)\n",
    "    score = davies_bouldin_score(X, labels)\n",
    "    scores_kmeans.append(score)\n",
    "\n",
    "visualize_optimization_results(\n",
    "    X, \n",
    "    cluster_range, \n",
    "    scores_kmeans, \n",
    "    kmeans_cluster_data, \n",
    "    algorithm_name=\"KMeans\", \n",
    "    score_func=davies_bouldin_score, \n",
    "    score_name=\"Davies-Bouldin\", \n",
    "    maximize=False\n",
    ")\n",
    "\n",
    "scores_agg = []\n",
    "for k in cluster_range:\n",
    "    labels = cluster_agglomerative(X, k)\n",
    "    score = davies_bouldin_score(X, labels)\n",
    "    scores_agg.append(score)\n",
    "\n",
    "visualize_optimization_results(\n",
    "    X, \n",
    "    cluster_range, \n",
    "    scores_agg, \n",
    "    cluster_agglomerative, \n",
    "    algorithm_name=\"Agglomerative Clustering\", \n",
    "    score_func=davies_bouldin_score, \n",
    "    score_name=\"Davies-Bouldin\", \n",
    "    maximize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calinski-Harabasz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Calinski-Harabasz Index (CH)**, also known as the Variance Ratio Criterion, evaluates clustering quality by comparing the dispersion between clusters to the dispersion within clusters. It is given by:\n",
    "$$\n",
    "CH = \\frac{Tr(B_k) / (k - 1)}{Tr(W_k) / (n - k)}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\(Tr(B_k)\\) is the trace of the between-cluster dispersion matrix,  \n",
    "- \\(Tr(W_k)\\) is the trace of the within-cluster dispersion matrix,  \n",
    "- \\(k\\) is the number of clusters,  \n",
    "- \\(n\\) is the total number of data points.\n",
    "\n",
    "**Interpretation:**  \n",
    "A higher Calinski-Harabasz Index indicates better clustering performance, as it implies a higher degree of separation between clusters relative to their compactness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "cluster_range = range(2, 10)\n",
    "\n",
    "\n",
    "scores_ch_kmeans = []\n",
    "for k in cluster_range:\n",
    "    labels = kmeans_cluster_data(X, k)\n",
    "    score = calinski_harabasz_score(X, labels)\n",
    "    scores_ch_kmeans.append(score)\n",
    "\n",
    "visualize_optimization_results(\n",
    "    X, \n",
    "    cluster_range, \n",
    "    scores_ch_kmeans, \n",
    "    cluster_agglomerative, \n",
    "    algorithm_name=\"k-means Clustering\", \n",
    "    score_func=calinski_harabasz_score, \n",
    "    score_name=\"Calinski-Harabasz Index\", \n",
    "    maximize=True\n",
    ")\n",
    "\n",
    "scores_ch_agg = []\n",
    "for k in cluster_range:\n",
    "    labels = cluster_agglomerative(X, k)\n",
    "    score = calinski_harabasz_score(X, labels)\n",
    "    scores_ch_agg.append(score)\n",
    "\n",
    "\n",
    "visualize_optimization_results(\n",
    "    X, \n",
    "    cluster_range, \n",
    "    scores_ch_agg, \n",
    "    cluster_agglomerative, \n",
    "    algorithm_name=\"Agglomerative Clustering\", \n",
    "    score_func=calinski_harabasz_score, \n",
    "    score_name=\"Calinski-Harabasz Index\", \n",
    "    maximize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks utills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_subdatasets(dataset_path, subsets_amount, subset_size):\n",
    "    \"\"\"\n",
    "    Reads a dataset from a path and splits it into smaller dataframes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_path : str\n",
    "        Path to the dataset file\n",
    "    subsets_amount : int\n",
    "        Number of subdatasets to create\n",
    "    subset_size : int\n",
    "        Size of each subdataset\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with dataset names as keys and pandas DataFrames as values\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not read dataset from {dataset_path}. Error: {str(e)}\")\n",
    "    \n",
    "    total_required_rows = subsets_amount * subset_size\n",
    "    if len(df) < total_required_rows:\n",
    "        raise ValueError(f\"Dataset has {len(df)} rows, but {total_required_rows} rows are required for {subsets_amount} subsets of size {subset_size}\")\n",
    "    \n",
    "    dataset_name = os.path.splitext(os.path.basename(dataset_path))[0]\n",
    "    \n",
    "    subdatasets = {}\n",
    "    for i in range(subsets_amount):\n",
    "        subset = df.sample(n=subset_size, random_state=i)\n",
    "        subset_name = f\"{dataset_name}_subset_{subset_size}_{i}\"\n",
    "        subdatasets[subset_name] = subset\n",
    "    \n",
    "    return subdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "def clustering_evaluation(X, labels):\n",
    "    \n",
    "    if len(set(labels)) > 1 and len(set(labels)) < len(X):\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "        davies_bouldin = davies_bouldin_score(X, labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(X, labels)\n",
    "    else:\n",
    "        silhouette = float('nan')\n",
    "        davies_bouldin = float('nan')\n",
    "        calinski_harabasz = float('nan')\n",
    "    \n",
    "    return silhouette, davies_bouldin, calinski_harabasz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def clustering_benchmark(models_dict, dataframes_dict, benchmark_name='Clustering Benchmark'):    \n",
    "    results = []\n",
    "    total_iterations = len(dataframes_dict) * len(models_dict)\n",
    "\n",
    "    with tqdm(total=total_iterations, desc='Clustering Progress') as pbar:\n",
    "        for dataset_name, df in dataframes_dict.items():\n",
    "            embed_cols = [col for col in df.columns if re.match(r'embed_dim_\\d+', col)]\n",
    "            \n",
    "            for model_name, model in models_dict.items():\n",
    "                model.fit(df[embed_cols])\n",
    "                labels = model.labels_\n",
    "                df['Cluster_Assignment'] = labels\n",
    "                \n",
    "                silhouette, davies_bouldin, calinski_harabasz = clustering_evaluation(df[embed_cols], labels)\n",
    "                \n",
    "                results.append({\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Dataset Size': len(df),\n",
    "                    'Model': model_name,\n",
    "                    'Silhouette Score': silhouette,\n",
    "                    'Davies-Bouldin Index': davies_bouldin,\n",
    "                    'Calinski-Harabasz Index': calinski_harabasz,\n",
    "                    'Number of Clusters': len(set(labels)) if hasattr(model, 'labels_') else 0\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                \n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    output_file= f\"./benchmarks results/{benchmark_name}.csv\"\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    return results_df, output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step, lets just put our points on the plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"./datasets/with-assumptions/jack_vs_calley_1000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "    \n",
    "df['x'] = df['embed_dim_0']\n",
    "df['y'] = df['embed_dim_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_scatter(df):        \n",
    "    fig = px.scatter(\n",
    "        df, \n",
    "        x='x', \n",
    "        y='y',\n",
    "        hover_data=['text'],\n",
    "        title='Visualization of Encoded Points',\n",
    "        labels={'x': 'Dimension 1', 'y': 'Dimension 2'},\n",
    "        opacity=0.7\n",
    "    )\n",
    "\n",
    "    # Improve layout\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        width=900,\n",
    "        height=700\n",
    "    )\n",
    "\n",
    "    # Add grid lines\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not see any pattern at this momnet.\n",
    "To make it more easly ilustrated, lets just pick 50 random comments.\n",
    "Also, we can assume the data is that clusted cuz of the huge dimensionality reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans, HDBSCAN\n",
    "\n",
    "data_path = \"./datasets/with-assumptions/jack_vs_calley_1000.csv\"\n",
    "\n",
    "benchmark_data = create_subdatasets(data_path,20, 50)\n",
    "\n",
    "models_dict = {\n",
    "    \"Agglomerative\": AgglomerativeClustering(n_clusters=5),\n",
    "    \"HDBSCAN\": HDBSCAN(min_cluster_size=3),\n",
    "    \"KMeans\": KMeans(n_clusters=5, random_state=42)\n",
    "}\n",
    "\n",
    "\n",
    "results_df, path = clustering_benchmark(models_dict, benchmark_data,benchmark_name=\"jack_vs_cally_small\")\n",
    "\n",
    "results_df.groupby('Model').mean(numeric_only=True).style\\\n",
    "    .format('{:.2f}')\\\n",
    "    .set_caption('Clustering Benchmark Results')\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERTopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
