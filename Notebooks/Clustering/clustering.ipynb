{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic based clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our project is to group YouTube comments together based on their topics.\n",
    "\n",
    "This notebook will explore different clustering algorithms, benchmark them, and recommend one that is most suitable for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem and the goal\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "We need an automated way to group YouTube video comments by their underlying topics based on text embeddings.\n",
    "\n",
    "Input challageds:\n",
    "- dynamic\n",
    "- no pre-defined labels\n",
    "- unknown number of natural topics (`k`)\n",
    "- requires fast processing for a user-facing application.\n",
    "\n",
    "**The Goal:**\n",
    "\n",
    "This notebook explores and implements clustering strategies that can rapidly process comment embeddings \"on the fly\". The aim is to find methods that quickly identify meaningful topic clusters for any video. \n",
    "\n",
    "Optimizing for internal validation scores:\n",
    " - Silhouette\n",
    " - Davies-Bouldin\n",
    " - Calinski-Harabasz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes the following: \n",
    "- Comments have already been cleaned and encoded. \n",
    "- Comments encoding have been reduced into a 2 dimensional space.\n",
    "\n",
    "Currently, we did not prefect these steps, so here is a short and imperfect implementation of these so we can start working. [link to notebook](./assumptions.ipynb)\n",
    "\n",
    "> We assume that the data is encoded properly and that by mesuring the distance between comments, we can cluster them based on topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Benchmarks\n",
    "- Davies-Bouldin\n",
    "- Silhouette\n",
    "- Calinski-Harabasz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install scikit-learn pandas numpy tqdm python-dotenv google-genai plotly matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering algorithms can be grouped into 4 main categories.\n",
    "\n",
    "![Clustering Algorithm Categories](./images/clustering%20groups.png)\n",
    "\n",
    "1.  **Flat vs. Hierarchical Output:** Does the algorithm produce a single partition or a nested structure?\n",
    "2.  **Core Mechanism:** Is it based on distance to centers (**Centroid/Parametric**) or on data point density (**Density/Non-Parametric**)?\n",
    "\n",
    "Understanding the strengths of these categories helps guide algorithm selection:\n",
    "\n",
    "* **Flat & Centroid/Parametric (e.g., KMeans, GMM):**\n",
    "    * **Strength:** Efficient partitioning into a predefined number (`k`) of typically globular/elliptical clusters. Good for large datasets when `k` is known and shapes are simple.\n",
    "* **Hierarchical & Centroid/Parametric (e.g., Ward, Agglomerative Linkages):**\n",
    "    * **Strength:** Reveals nested cluster structures (dendrogram), useful for exploring different granularities without fixing `k` initially. Ward linkage is good for finding compact, balanced clusters within the hierarchy.\n",
    "* **Flat & Density/Non-Parametric (e.g., DBSCAN, Mean Shift):**\n",
    "    * **Strength:** Discovering arbitrarily shaped clusters and identifying noise without needing `k` upfront. Effective when density defines groups better than proximity to a center.\n",
    "* **Hierarchical (Internal) & Density/Non-Parametric (e.g., HDBSCAN):**\n",
    "    * **Strength:** Robustly finds arbitrarily shaped clusters of varying densities, handles noise, and automatically determines a suitable number of clusters (`k`) based on stability. Excellent for exploration when `k` and shapes are unknown. (Note: Outputs a flat partition).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(n_samples=500, random_state=76):\n",
    "    datasets_dict = {}\n",
    "\n",
    "    X_blobs, y_blobs = datasets.make_blobs(\n",
    "        n_samples=n_samples, centers=3, cluster_std=0.8, random_state=random_state\n",
    "    )\n",
    "    datasets_dict['Blobs'] = (X_blobs, y_blobs)\n",
    "\n",
    "    X_moons, y_moons = datasets.make_moons(\n",
    "        n_samples=n_samples, noise=0.05, random_state=random_state\n",
    "    )\n",
    "    datasets_dict['Moons'] = (X_moons, y_moons)\n",
    "\n",
    "    X_circles, y_circles = datasets.make_circles(\n",
    "        n_samples=n_samples, factor=0.5, noise=0.05, random_state=random_state\n",
    "    )\n",
    "    datasets_dict['Circles'] = (X_circles, y_circles)\n",
    "\n",
    "    X_varied, y_varied = datasets.make_blobs(\n",
    "        n_samples=n_samples,\n",
    "        centers=[[1, 1], [-2, -2], [3, -1]],\n",
    "        cluster_std=[0.5, 1.5, 0.3],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    datasets_dict['Varied Blobs'] = (X_varied, y_varied)\n",
    "\n",
    "    X_aniso, y_aniso = datasets.make_blobs(\n",
    "        n_samples=n_samples, centers=3, random_state=random_state\n",
    "    )\n",
    "    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "    X_aniso = np.dot(X_aniso, transformation)\n",
    "    datasets_dict['Anisotropic Blobs'] = (X_aniso, y_aniso)\n",
    "\n",
    "    X_noise = np.random.rand(n_samples, 2) * 10\n",
    "    datasets_dict['No Structure (Noise)'] = (X_noise, None)\n",
    "\n",
    "    print(f\"Generated datasets: {list(datasets_dict.keys())}\")\n",
    "    return datasets_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating sample datasets...\")\n",
    "sample_datasets = generate_datasets(n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, labels, title=\"Clustering Results\", noise_color='lightgrey', palette='viridis', noise_marker='x', figsize=(8, 6)):\n",
    "    if X.shape[1] != 2:\n",
    "        print(\"Warning: Plotting function currently only supports 2D data.\")\n",
    "        return\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    if n_clusters > 0:\n",
    "        colors = sns.color_palette(palette, n_colors=n_clusters)\n",
    "        label_color_map = {label: colors[i] for i, label in enumerate(sorted(l for l in unique_labels if l != -1))}\n",
    "    else:\n",
    "        label_color_map = {}\n",
    "\n",
    "    label_color_map[-1] = noise_color\n",
    "\n",
    "    is_noise = (labels == -1)\n",
    "    if np.any(is_noise):\n",
    "        plt.scatter(X[is_noise, 0], X[is_noise, 1], c=label_color_map[-1], marker=noise_marker, label='Noise', alpha=0.5, s=30, zorder=1)\n",
    "\n",
    "    for label, color in label_color_map.items():\n",
    "        if label == -1: continue\n",
    "        cluster_points = X[labels == label]\n",
    "        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], color=color, label=f'Cluster {label}', alpha=0.8, s=50, zorder=2)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    handles, current_labels = plt.gca().get_legend_handles_labels()\n",
    "    if handles:\n",
    "      plt.legend(scatterpoints=1, loc='best', ncol=1, fontsize=8)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_datasets(datasets_dict, model, model_name, grid_shape=(2, 3), figsize=(12, 9), point_size=10, cmap='viridis'):\n",
    "    rows, cols = grid_shape\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    dataset_items = list(datasets_dict.items())\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < len(dataset_items):\n",
    "            name, (X, y_true) = dataset_items[i]\n",
    "\n",
    "            try:\n",
    "                model.fit(X)\n",
    "                y_pred = model.labels_\n",
    "            except Exception as e:\n",
    "                print(f\"Error fitting model to dataset '{name}': {e}\")\n",
    "                y_pred = None\n",
    "\n",
    "            if y_pred is not None:\n",
    "                unique_labels = set(y_pred)\n",
    "                if -1 in unique_labels:\n",
    "                    noise_mask = (y_pred == -1)\n",
    "                    scatter = ax.scatter(X[~noise_mask, 0], X[~noise_mask, 1], c=y_pred[~noise_mask], \n",
    "                                        cmap='viridis', s=point_size, alpha=0.7)\n",
    "                    ax.scatter(X[noise_mask, 0], X[noise_mask, 1], c='gray', s=point_size, alpha=0.5)\n",
    "                else:\n",
    "                    scatter = ax.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=point_size, alpha=0.7)\n",
    "            else:\n",
    "                scatter = ax.scatter(X[:, 0], X[:, 1], s=point_size, alpha=0.7, color='gray')\n",
    "\n",
    "            ax.set_title(f\"{name}\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            # Set limits to exactly match the data range\n",
    "            x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "            y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "            \n",
    "            # Add minimal padding (1%) to prevent points from touching the edge\n",
    "            x_padding = 0.01 * (x_max - x_min)\n",
    "            y_padding = 0.01 * (y_max - y_min)\n",
    "            \n",
    "            ax.set_xlim(x_min - x_padding, x_max + x_padding)\n",
    "            ax.set_ylim(y_min - y_padding, y_max + y_padding)\n",
    "            ax.set_aspect('equal')\n",
    "            \n",
    "            # Keep borders visible\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(True)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Clustering Results using {model_name}\", fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring HDBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why HDBSCAN is a Good Fit Here\n",
    "\n",
    "HDBSCAN stands out as a strong candidate for clustering YouTube comment embeddings for several key reasons:\n",
    "\n",
    "* **Finds Number of Clusters (`k`) Automatically:** Unlike KMeans, it figures out the most stable number of clusters based on the data's density, which is essential when `k` is unknown and varies between videos.\n",
    "* **Handles Complex Clusters:** As a density-based method, it can find clusters of various shapes and densities, which is useful as topic clusters in embedding space might not be simple spheres.\n",
    "* **Identifies Noise:** It explicitly labels outlier or irrelevant comments as noise (-1), separating them from the core topic clusters.\n",
    "* **Robust & Flexible:** It generally requires less sensitive parameter tuning than similar methods like DBSCAN and fits well when we don't want to assume simple cluster shapes or a fixed `k`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How HDBSCAN Works (Conceptual Overview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "HDBSCAN refines density-based clustering by creating a cluster hierarchy and selecting the most *stable* groups. Here’s the essence:\n",
    "\n",
    "1.  **Adjust Distances based on Density:**\n",
    "    * Calculates **core distance** for each point `x` (distance to its k-th neighbor, reflecting local density):\n",
    "        $$ \\text{core}_k(x) = d(x, \\text{k-th NN of } x) $$\n",
    "    * Computes **mutual reachability distance** between points `a` and `b`, using core distances to effectively shorten distances within dense regions compared to sparse ones:\n",
    "        $$ d_{\\text{mreach}-k}(a, b) = \\max(\\text{core}_k(a), \\text{core}_k(b), d(a, b)) $$\n",
    "\n",
    "![DBSCAN gif 1](./images/DBSCAN_GIF_1.gif) ![DBSCAN gif 2](./images/DBSCAN_gif_2.gif)\n",
    "\n",
    "\n",
    "2.  **Build Hierarchy from Dense Connections:**\n",
    "    * Constructs a Minimum Spanning Tree (MST) using mutual reachability distances, connecting points primarily through dense areas.\n",
    "    * Creates a hierarchy of potential clusters by simulating splits in the MST as the distance threshold increases (like varying `epsilon` in DBSCAN*).\n",
    "\n",
    "    ![hiracical gif](./images/hierarch_gif_1.gif)\n",
    "\n",
    "3.  **Select Stable Clusters:**\n",
    "    * Prunes the hierarchy based on `min_cluster_size` (removing tiny, insignificant splits).\n",
    "    * **Key Step:** Measures cluster **stability** – essentially, how long a cluster persists across a range of density levels ($\\lambda = 1/\\text{distance}$). Stability can be approximated by summing individual point persistence within the cluster:\n",
    "        $$ \\text{Stability}(C) \\approx \\sum_{p \\in C} (\\lambda_{p, \\text{leaves}} - \\lambda_{C, \\text{birth}}) $$\n",
    "    * It extracts the clusters that maximize this stability metric. Points not part of any selected stable cluster are designated as noise (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "hdbscan_model_instance = HDBSCAN(\n",
    "    min_cluster_size=15,    \n",
    "    metric='euclidean',      \n",
    ")\n",
    "\n",
    "hdbscan_model_name = \"HDBSCAN (min_cluster_size=15)\"\n",
    "\n",
    "\n",
    "plot_all_datasets(\n",
    "    datasets_dict=sample_datasets,\n",
    "    model=hdbscan_model_instance,\n",
    "    model_name=hdbscan_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Mini Batch K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Mini batch k-means Might Be a Good Fit\n",
    "\n",
    "* **Speed:** This is its main advantage. It processes data in small batches, making it significantly faster than standard KMeans and often faster than density-based methods, especially as the number of comments increases. This directly addresses the \"on the fly\" processing requirement.\n",
    "* **Scalability:** Uses much less memory than batch KMeans, making it suitable even if a video has a very large number of comments.\n",
    "* **Simplicity:** It's a variation of the well-understood KMeans algorithm.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "* **Requires `k`:** Like standard KMeans, you must specify the number of clusters (`k`). Our strategy would involve quickly testing a small, predefined set of `k` values within the `fit` process and choosing the best based on internal metrics.\n",
    "* **Cluster Shape:** It assumes clusters are roughly spherical, which might be a limitation compared to HDBSCAN if comment topics form more complex shapes in the embedding space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Mini batch k-means works (Conceptual Overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  **Initialize Centroids:** Starts by initializing `k` cluster centroids, similar to standard KMeans (e.g., randomly or using k-means++).\n",
    "\n",
    "2.  **Iterative Refinement using Mini-Batches:** This is where it differs significantly from standard KMeans:\n",
    "    * **a) Draw Mini-Batch:** Instead of using the *entire dataset* in each step, it randomly samples a small subset of the data called a **mini-batch**.\n",
    "    * **b) Assign Batch Points:** Points *within the current mini-batch* are assigned to their nearest centroid.\n",
    "    * **c) Update Affected Centroids:** The positions of the centroids that received points from the mini-batch are updated. Unlike standard KMeans which recalculates the mean using *all* points at the end of an iteration, MiniBatchKMeans performs an *online update*. For each point `x` in the mini-batch assigned to centroid `c_i`:\n",
    "        * The algorithm typically maintains a count `v_i` of points assigned to cluster `i` so far. Increment `v_i`.\n",
    "        * Calculate a per-point learning rate, often `\\eta = 1 / v_i`.\n",
    "        * Update the centroid `c_i` by moving it slightly towards the new point `x`:\n",
    "            $$ c_i \\leftarrow (1 - \\eta) c_i + \\eta x $$\n",
    "        This performs a running average, giving less weight to newer points as more points have been assigned to the cluster, helping stabilize the centroid position.\n",
    "        \n",
    "3.  **Repeat:** Steps 2a-2c are repeated with new random mini-batches until the centroids stabilize or a maximum number of iterations is reached.\n",
    "\n",
    "![kmeans_gif](./images/kmeans_gif.gif)\n",
    "\n",
    "\n",
    "**Key Differences from Standard KMeans:**\n",
    "* **Data per Iteration:** Uses small batches vs. the full dataset.\n",
    "* **Centroid Update:** Based on batch points (+ learning rate) vs. mean of all assigned points.\n",
    "* **Result:** Significantly faster and uses less memory, but the final centroid positions (and cluster assignments) might be slightly different and the inertia (sum of squared distances) potentially higher than batch KMeans due to the approximation. However, results are often very similar in practice.\n",
    "\n",
    "![mini_batch_vs_kmeans](./images/mini_batch_kmeans_vs_kmeans.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "mbk_model = MiniBatchKMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters=3,\n",
    "    batch_size=75,\n",
    "    n_init=10,\n",
    "    max_no_improvement=5,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "mkb_model_name = \"mini-batch-k-means\"\n",
    "\n",
    "plot_all_datasets(\n",
    "    datasets_dict=sample_datasets,\n",
    "    model=mbk_model,\n",
    "    model_name=mkb_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering preformance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we explore how to evaluate clustering results when we have no true labels. We generate a synthetic dataset, visualize it using Plotly, then cluster it in different ways. The key question is: **How will we know which clustering is the best?**  \n",
    "We answer that by comparing evaluation metrics:\n",
    "- **Silhouette Score**: Measures how similar a point is to its own cluster versus other clusters (range: -1 to 1; higher is better).\n",
    "- **Davies-Bouldin Index**: Quantifies the average similarity between each cluster and its most similar one (lower is better).\n",
    "- **Calinski-Harabasz Index**: Computes the ratio of between-cluster dispersion to within-cluster dispersion (higher is better).\n",
    "\n",
    "We also show how to optimize the clustering (i.e., choosing the number of clusters) using these metrics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Setup and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data(n_samples=400, centers=5, random_state=20):\n",
    "    \"\"\"Generate synthetic data using make_blobs.\"\"\"\n",
    "    X, _ = make_blobs(n_samples=n_samples, centers=centers, random_state=random_state)\n",
    "    return X\n",
    "\n",
    "def plot_data(X, labels=None, title=\"Data Visualization\"):\n",
    "    \"\"\"Plot data using Plotly.\"\"\"\n",
    "    if labels is None:\n",
    "        fig = px.scatter(x=X[:, 0], y=X[:, 1], title=title)\n",
    "    else:\n",
    "        fig = px.scatter(x=X[:, 0], y=X[:, 1], color=labels.astype(str), title=title)\n",
    "    fig.show()\n",
    "\n",
    "X = generate_data()\n",
    "plot_data(X, title=\"Original Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def kmeans_cluster_data(X, n_clusters):\n",
    "    \"\"\"Cluster data using KMeans.\"\"\"\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = model.fit_predict(X)\n",
    "    return labels\n",
    "\n",
    "def cluster_agglomerative(X, n_clusters):\n",
    "    model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    return model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_fixed = kmeans_cluster_data(X, n_clusters=3)\n",
    "plot_data(X, labels_fixed, title=\"k-means Clustering with 3 Clusters\")\n",
    "\n",
    "labels_fixed = kmeans_cluster_data(X, n_clusters=5)\n",
    "plot_data(X, labels_fixed, title=\"k-means Clustering with 5 Clusters\")\n",
    "\n",
    "labels_fixed = kmeans_cluster_data(X, n_clusters=10)\n",
    "plot_data(X, labels_fixed, title=\"k-means Clustering with 10 Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_fixed = cluster_agglomerative(X, n_clusters=3)\n",
    "plot_data(X, labels_fixed, title=\"agglomerative Clustering with 3 Clusters\")\n",
    "\n",
    "labels_fixed = cluster_agglomerative(X, n_clusters=5)\n",
    "plot_data(X, labels_fixed, title=\"agglomerative Clustering with 5 Clusters\")\n",
    "\n",
    "labels_fixed = cluster_agglomerative(X, n_clusters=10)\n",
    "plot_data(X, labels_fixed, title=\"agglomerative Clustering with 10 Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize_optimization_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "def visualize_optimization_results(X, cluster_range, scores, cluster_func, algorithm_name, score_func, score_name, maximize=True):\n",
    "    \"\"\"\n",
    "    Visualizes clustering optimization results for any evaluation metric.\n",
    "\n",
    "    It plots:\n",
    "    - The evaluation score versus the number of clusters.\n",
    "    - Clustering visualizations for three key cases:\n",
    "      * Initial (first value in cluster_range)\n",
    "      * Middle (middle value in cluster_range)\n",
    "      * Best (based on the provided evaluation metric)\n",
    "    \n",
    "    Parameters:\n",
    "        X : np.array\n",
    "            The data to be clustered.\n",
    "        cluster_range : list or range\n",
    "            A sequence of cluster counts that were evaluated.\n",
    "        scores : list of float\n",
    "            The evaluation scores corresponding to each cluster count.\n",
    "        cluster_func : function\n",
    "            A clustering function that accepts (X, n_clusters) and returns cluster labels.\n",
    "        algorithm_name : str\n",
    "            The name of the clustering algorithm (for labeling plots).\n",
    "        score_func : function\n",
    "            A function that calculates the evaluation score given (X, labels).\n",
    "        score_name : str\n",
    "            The name of the evaluation metric (e.g., \"Silhouette Score\").\n",
    "        maximize : bool (default=True)\n",
    "            Set True if a higher score is better (e.g., Silhouette Score), or False if lower is better (e.g., Davies-Bouldin Index).\n",
    "    \"\"\"\n",
    "    # Convert cluster_range to a list (if not already)\n",
    "    cluster_range = list(cluster_range)\n",
    "    \n",
    "    # Determine key points: initial, middle, and best cluster counts.\n",
    "    initial_k = cluster_range[0]\n",
    "    middle_index = len(cluster_range) // 2\n",
    "    middle_k = cluster_range[middle_index]\n",
    "    best_index = np.argmax(scores) if maximize else np.argmin(scores)\n",
    "    best_k = cluster_range[best_index]\n",
    "    \n",
    "    # Print details\n",
    "    print(f\"{algorithm_name} Optimization for {score_name}:\")\n",
    "    print(f\"Initial: {initial_k} clusters, {score_name}: {scores[0]:.3f}\")\n",
    "    print(f\"Middle: {middle_k} clusters, {score_name}: {scores[middle_index]:.3f}\")\n",
    "    print(f\"Best: {best_k} clusters, {score_name}: {scores[best_index]:.3f}\")\n",
    "    \n",
    "    # Plot the optimization graph using matplotlib.\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(cluster_range, scores, marker='o', label=algorithm_name)\n",
    "    plt.scatter([initial_k, middle_k, best_k],\n",
    "                [scores[0], scores[middle_index], scores[best_index]],\n",
    "                color='red', zorder=5, label='Selected Points')\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(score_name)\n",
    "    plt.title(f\"{algorithm_name} Optimization Graph ({score_name})\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize clustering results for the initial, middle, and best cases using Plotly.\n",
    "    for k, label in zip([initial_k, middle_k, best_k], [\"Initial\", \"Middle\", \"Best\"]):\n",
    "        cluster_labels = cluster_func(X, k)\n",
    "        current_score = score_func(X, cluster_labels)\n",
    "        fig = px.scatter(x=X[:, 0], y=X[:, 1], color=cluster_labels.astype(str),\n",
    "                         title=f\"{algorithm_name} {label} Clustering: {k} Clusters ({score_name} = {current_score:.3f})\")\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know wich clustring algorithm is the best? what are the best hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Silhouette Score** measures how similar a data point is to its own cluster compared to other clusters. For each sample \\(i\\), the silhouette score is given by:\n",
    "\n",
    "$$\n",
    "s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\(a(i)\\) is the average distance between \\(i\\) and all other points in the same cluster,  \n",
    "- \\(b(i)\\) is the minimum average distance from \\(i\\) to points in a different cluster (i.e., the closest cluster that \\(i\\) is not a member of).\n",
    "\n",
    "A higher Silhouette Score (close to 1) indicates that the sample is well matched to its own cluster and poorly matched to neighboring clusters. We use this metric to optimize the clustering (by tuning the number of clusters) and to compare different clustering algorithms.\n",
    "\n",
    ">  NOTE: Clusters might overlap and it alright in our use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "cluster_range = range(2, 10)\n",
    "\n",
    "scores_kmeans = []\n",
    "for k in cluster_range:\n",
    "    labels = kmeans_cluster_data(X, k)\n",
    "    score = silhouette_score(X, labels)\n",
    "    scores_kmeans.append(score)\n",
    "\n",
    "visualize_optimization_results(\n",
    "    X, \n",
    "    cluster_range, \n",
    "    scores_kmeans, \n",
    "    kmeans_cluster_data, \n",
    "    algorithm_name=\"KMeans\", \n",
    "    score_func=silhouette_score, \n",
    "    score_name=\"Silhouette Score\", \n",
    "    maximize=True\n",
    ")\n",
    "\n",
    "scores_agg = []\n",
    "for k in cluster_range:\n",
    "    labels = cluster_agglomerative(X, k)\n",
    "    score = silhouette_score(X, labels)\n",
    "    scores_agg.append(score)\n",
    "\n",
    "visualize_optimization_results(\n",
    "    X, \n",
    "    cluster_range, \n",
    "    scores_agg, \n",
    "    cluster_agglomerative, \n",
    "    algorithm_name=\"Agglomerative Clustering\", \n",
    "    score_func=silhouette_score, \n",
    "    score_name=\"Silhouette Score\", \n",
    "    maximize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Bouldin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Davies-Bouldin Index (DB)** measures the average similarity between each cluster and its most similar (i.e., most overlapping) cluster. It is defined as:\n",
    "\n",
    "$$\n",
    "DB = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\neq i} \\left(\\frac{s_i + s_j}{d(c_i, c_j)}\\right)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\(k\\) is the number of clusters,  \n",
    "- \\(s_i\\) is the average distance between each point in cluster \\(i\\) and the centroid of cluster \\(i\\) (a measure of cluster compactness),  \n",
    "- \\(d(c_i, c_j)\\) is the distance between the centroids of clusters \\(i\\) and \\(j\\).\n",
    "\n",
    "**Interpretation:**  \n",
    "A lower Davies-Bouldin Index indicates better clustering performance, as it suggests that clusters are compact and well separated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "cluster_range = range(2, 10)\n",
    "\n",
    "scores_kmeans = []\n",
    "for k in cluster_range:\n",
    "    labels = kmeans_cluster_data(X, k)\n",
    "    score = davies_bouldin_score(X, labels)\n",
    "    scores_kmeans.append(score)\n",
    "\n",
    "visualize_optimization_results(\n",
    "    X, \n",
    "    cluster_range, \n",
    "    scores_kmeans, \n",
    "    kmeans_cluster_data, \n",
    "    algorithm_name=\"KMeans\", \n",
    "    score_func=davies_bouldin_score, \n",
    "    score_name=\"Davies-Bouldin\", \n",
    "    maximize=False\n",
    ")\n",
    "\n",
    "scores_agg = []\n",
    "for k in cluster_range:\n",
    "    labels = cluster_agglomerative(X, k)\n",
    "    score = davies_bouldin_score(X, labels)\n",
    "    scores_agg.append(score)\n",
    "\n",
    "visualize_optimization_results(\n",
    "    X, \n",
    "    cluster_range, \n",
    "    scores_agg, \n",
    "    cluster_agglomerative, \n",
    "    algorithm_name=\"Agglomerative Clustering\", \n",
    "    score_func=davies_bouldin_score, \n",
    "    score_name=\"Davies-Bouldin\", \n",
    "    maximize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calinski-Harabasz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Calinski-Harabasz Index (CH)**, also known as the Variance Ratio Criterion, evaluates clustering quality by comparing the dispersion between clusters to the dispersion within clusters. It is given by:\n",
    "$$\n",
    "CH = \\frac{Tr(B_k) / (k - 1)}{Tr(W_k) / (n - k)}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\(Tr(B_k)\\) is the trace of the between-cluster dispersion matrix,  \n",
    "- \\(Tr(W_k)\\) is the trace of the within-cluster dispersion matrix,  \n",
    "- \\(k\\) is the number of clusters,  \n",
    "- \\(n\\) is the total number of data points.\n",
    "\n",
    "**Interpretation:**  \n",
    "A higher Calinski-Harabasz Index indicates better clustering performance, as it implies a higher degree of separation between clusters relative to their compactness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "cluster_range = range(2, 10)\n",
    "\n",
    "\n",
    "scores_ch_kmeans = []\n",
    "for k in cluster_range:\n",
    "    labels = kmeans_cluster_data(X, k)\n",
    "    score = calinski_harabasz_score(X, labels)\n",
    "    scores_ch_kmeans.append(score)\n",
    "\n",
    "visualize_optimization_results(\n",
    "    X, \n",
    "    cluster_range, \n",
    "    scores_ch_kmeans, \n",
    "    cluster_agglomerative, \n",
    "    algorithm_name=\"k-means Clustering\", \n",
    "    score_func=calinski_harabasz_score, \n",
    "    score_name=\"Calinski-Harabasz Index\", \n",
    "    maximize=True\n",
    ")\n",
    "\n",
    "scores_ch_agg = []\n",
    "for k in cluster_range:\n",
    "    labels = cluster_agglomerative(X, k)\n",
    "    score = calinski_harabasz_score(X, labels)\n",
    "    scores_ch_agg.append(score)\n",
    "\n",
    "\n",
    "visualize_optimization_results(\n",
    "    X, \n",
    "    cluster_range, \n",
    "    scores_ch_agg, \n",
    "    cluster_agglomerative, \n",
    "    algorithm_name=\"Agglomerative Clustering\", \n",
    "    score_func=calinski_harabasz_score, \n",
    "    score_name=\"Calinski-Harabasz Index\", \n",
    "    maximize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary And Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When clustering YouTube comments, we face challenges like dynamic data, no pre-defined labels, an unknown number of topics (`k`), and the need for fast processing. Here's a summary of the metrics we considered:\n",
    "\n",
    "#### Silhouette Coefficient\n",
    "\n",
    "| Pros                                     | Cons                                     |\n",
    "| :--------------------------------------- | :--------------------------------------- |\n",
    "| Provides granular, per-sample evaluation | Sensitive to cluster shape and density   |\n",
    "| Intuitive range (-1 to +1)              | Computationally moderately expensive   |\n",
    "| Robust measure of cohesion and separation |                                          |\n",
    "\n",
    "#### Davies-Bouldin Index\n",
    "\n",
    "| Pros                                     | Cons                                         |\n",
    "| :--------------------------------------- | :------------------------------------------- |\n",
    "| Emphasizes cluster separation             | Assumes convex clusters                    |\n",
    "| Computationally efficient               | Sensitive to noise and outliers            |\n",
    "| Provides a global cluster quality measure | Lower values indicate better clustering (less intuitive) |\n",
    "\n",
    "#### Calinski-Harabasz Index\n",
    "\n",
    "| Pros                                     | Cons                                           |\n",
    "| :--------------------------------------- | :--------------------------------------------- |\n",
    "| Computationally very efficient            | Very sensitive to the number of clusters (`k`) |\n",
    "| Provides a global cluster quality measure | Assumes convex clusters                      |\n",
    "| Useful for determining optimal `k`        | Sensitive to cluster shape and density       |\n",
    "\n",
    "#### Decision: Weighted Average of Silhouette and Davies-Bouldin\n",
    "\n",
    "Given our constraints, we will proceed with a weighted average of the Silhouette and Davies-Bouldin indices.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "* **Complementary Strengths:** Silhouette provides a detailed view of individual sample clustering, while Davies-Bouldin emphasizes overall cluster separation.\n",
    "* **Robustness:** Combining them mitigates the weaknesses of each individual metric.\n",
    "* **Relevance:** Both metrics are applicable to our challenge of unknown `k`, as they help assess clustering quality without reliance on pre-defined labels.\n",
    "* **Computational Efficiency:** While Silhouette is moderately expensive, Davies-Bouldin is efficient. And when used together, they are still fast enough for a user facing application.\n",
    "* **Normalization:** We will normalize both metrics to a common scale (0 to 1) to ensure they contribute equally to the average.\n",
    "* **Weighted:** We can add weights to the average, if we determine that one of the metrics is more important to our specific use case.\n",
    "* **DBSCAN/HDBSCAN Compatibility:** Suitable for evaluating DBSCAN and HDBSCAN, handling non-convex clusters and noise points effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks utills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_subdatasets(dataset_path, subsets_amount, subset_size):\n",
    "    \"\"\"\n",
    "    Reads a dataset from a path and splits it into smaller dataframes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_path : str\n",
    "        Path to the dataset file\n",
    "    subsets_amount : int\n",
    "        Number of subdatasets to create\n",
    "    subset_size : int\n",
    "        Size of each subdataset\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with dataset names as keys and pandas DataFrames as values\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not read dataset from {dataset_path}. Error: {str(e)}\")\n",
    "    \n",
    "    total_required_rows = subsets_amount * subset_size\n",
    "    if len(df) < total_required_rows:\n",
    "        raise ValueError(f\"Dataset has {len(df)} rows, but {total_required_rows} rows are required for {subsets_amount} subsets of size {subset_size}\")\n",
    "    \n",
    "    dataset_name = os.path.splitext(os.path.basename(dataset_path))[0]\n",
    "    \n",
    "    subdatasets = {}\n",
    "    for i in range(subsets_amount):\n",
    "        subset = df.sample(n=subset_size, random_state=i)\n",
    "        subset_name = f\"{dataset_name}_subset_{subset_size}_{i}\"\n",
    "        subdatasets[subset_name] = subset\n",
    "    \n",
    "    return subdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Loads all CSV files from a directory into a dictionary of pandas DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    directory_path : str\n",
    "        Path to the directory containing CSV files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with dataset names as keys and pandas DataFrames as values\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    if not os.path.exists(directory_path):\n",
    "        raise ValueError(f\"Directory {directory_path} does not exist\")\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {directory_path}\")\n",
    "        return datasets\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(directory_path, csv_file)\n",
    "        dataset_name = os.path.splitext(csv_file)[0]\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            datasets[dataset_name] = df\n",
    "            print(f\"Loaded {dataset_name} with {len(df)} rows and {len(df.columns)} columns\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {csv_file}: {str(e)}\")\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import numpy as np\n",
    "\n",
    "def clustering_evaluation(X, labels):\n",
    "    n_labels = len(set(labels))\n",
    "    if n_labels < 2 or n_labels >= X.shape[0]:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    try:\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "    except ValueError: silhouette = np.nan\n",
    "    try:\n",
    "        davies_bouldin = davies_bouldin_score(X, labels)\n",
    "    except ValueError: davies_bouldin = np.nan\n",
    "    try:\n",
    "        calinski_harabasz = calinski_harabasz_score(X, labels)\n",
    "    except ValueError: calinski_harabasz = np.nan\n",
    "    return silhouette, davies_bouldin, calinski_harabasz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(model_instance, X):\n",
    "    \"\"\"\n",
    "    Fits a clustering model instance and evaluates it using global\n",
    "    clustering_evaluation function.\n",
    "\n",
    "    Args:\n",
    "        model_instance: A scikit-learn compatible clustering model instance.\n",
    "        X (np.ndarray): Input data.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing 'silhouette', 'davies_bouldin',\n",
    "              'calinski_harabasz' scores. Returns NaNs on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clone to avoid modifying the original instance if it's stateful,\n",
    "        fitted_model = clone(model_instance)\n",
    "        labels = fitted_model.fit_predict(X)\n",
    "        sil, db, ch = clustering_evaluation(X, labels)\n",
    "        return {\n",
    "            'silhouette': sil,\n",
    "            'davies_bouldin': db,\n",
    "            'calinski_harabasz': ch\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        return {'silhouette': np.nan, 'davies_bouldin': np.nan, 'calinski_harabasz': np.nan}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_clustering_benchmark(df):\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for model in df['Model'].unique():\n",
    "        model_data = df[df['Model'] == model]\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            name=model,\n",
    "            x=model_data['Dataset'],\n",
    "            y=model_data['benchmark score'],\n",
    "            hovertemplate=(\n",
    "                'Dataset: %{x}<br>' +\n",
    "                'Benchmark Score: %{y:.3f}<br>' +\n",
    "                'Silhouette Score: %{customdata[0]:.3f}<br>' +\n",
    "                'Davies-Bouldin Index: %{customdata[1]:.3f}<br>' +\n",
    "                'Calinski-Harabasz Index: %{customdata[2]:.3f}<br>'\n",
    "            ),\n",
    "            customdata=model_data[['Silhouette Score', 'Davies-Bouldin Index', 'Calinski-Harabasz Index']].values\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Clustering Benchmark Results',\n",
    "        xaxis_title='Dataset',\n",
    "        yaxis_title='Benchmark Score',\n",
    "        barmode='group'\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def calculate_normalized_weighted_average(df, result_column_name, silhouette_weight=0.5, davies_bouldin_weight=0.5):\n",
    "    scaler = MinMaxScaler()\n",
    "    df['Normalized Silhouette Score'] = scaler.fit_transform(df[['Silhouette Score']])\n",
    "    df['Normalized Davies-Bouldin Index'] = 1 - scaler.fit_transform(df[['Davies-Bouldin Index']])\n",
    "\n",
    "    df[result_column_name] = (df['Normalized Silhouette Score'] * silhouette_weight) + (df['Normalized Davies-Bouldin Index'] * davies_bouldin_weight)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  **Normalization:**\n",
    "    * **MinMaxScaler:**\n",
    "        * This scikit-learn tool transforms data to a specific range (by default, 0 to 1).\n",
    "        * It scales each value proportionally based on the minimum and maximum values found in the column.\n",
    "    * **Silhouette Score:**\n",
    "        * Normalized directly to 0-1, as higher scores are better.\n",
    "    * **Davies-Bouldin Index:**\n",
    "        * Normalized to 0-1, but then inverted (`1 - normalized_value`). This is because *lower* Davies-Bouldin scores are better, so we want to reverse the scale.\n",
    "\n",
    "2.  **Combined Score:**\n",
    "    * It's a weighted average of the *normalized* scores.\n",
    "    * This ensures that both metrics contribute fairly, regardless of their original scales.\n",
    "    * The weights allow you to prioritize one metric over the other.\n",
    "    * Because both normalized scores are between 0 and 1, the combined score is also between 0 and 1.\n",
    "    * The combined score is intended to give a single value that represents the overall quality of the clustering, taking into account both the compactness and the separation of the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def clustering_benchmark(models_dict, dataframes_dict, benchmark_name='Clustering Benchmark'):    \n",
    "    results = []\n",
    "    total_iterations = len(dataframes_dict) * len(models_dict)\n",
    "\n",
    "    with tqdm(total=total_iterations, desc='Clustering Progress') as pbar:\n",
    "        for dataset_name, df in dataframes_dict.items():\n",
    "            embed_cols = [col for col in df.columns if re.match(r'embed_dim_\\d+', col)]\n",
    "            \n",
    "            for model_name, model in models_dict.items():\n",
    "                model.fit(df[embed_cols])\n",
    "                labels = model.labels_\n",
    "                df['Cluster_Assignment'] = labels\n",
    "                \n",
    "                silhouette, davies_bouldin, calinski_harabasz = clustering_evaluation(df[embed_cols], labels)\n",
    "                \n",
    "                results.append({\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Dataset Size': len(df),\n",
    "                    'Model': model_name,\n",
    "                    'Silhouette Score': silhouette,\n",
    "                    'Davies-Bouldin Index': davies_bouldin,\n",
    "                    'Calinski-Harabasz Index': calinski_harabasz,\n",
    "                    'Number of Clusters': len(set(labels)) if hasattr(model, 'labels_') else 0\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                \n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    output_file= f\"./benchmarks results/{benchmark_name}.csv\"\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    return results_df, output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans, HDBSCAN, MeanShift, AgglomerativeClustering,DBSCAN\n",
    "\n",
    "data_path = \"./datasets/with-assumptions\"\n",
    "\n",
    "benchmark_data = load_datasets_from_directory(data_path)\n",
    "\n",
    "models_dict = {\n",
    "    \"MiniBatchKMeans\": MiniBatchKMeans(n_clusters=6, random_state=42, batch_size=200),\n",
    "    \"HDBSCAN\": HDBSCAN(min_cluster_size=2, min_samples=3),\n",
    "    \"DBSCAN\": DBSCAN(eps=0.01, min_samples=5),\n",
    "    \"Ward\": AgglomerativeClustering(n_clusters=6, linkage='ward')\n",
    "}\n",
    "\n",
    "results_df, path = clustering_benchmark(models_dict, benchmark_data, benchmark_name=\"allDatasets\")\n",
    "normalized = calculate_normalized_weighted_average(results_df, \"benchmark score\")\n",
    "\n",
    "display(normalized)\n",
    "\n",
    "plot_clustering_benchmark(normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cond",
   "language": "python",
   "name": "cond"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
