{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic based clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our project is to group YouTube comments together based on their topics.\n",
    "\n",
    "This notebook will explore different clustering algorithms, benchmark them, and recommend one that is most suitable for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes the following: \n",
    "- Comments have already been cleaned and encoded. \n",
    "- Comments encoding have been reduced into a 2 dimensional space.\n",
    "\n",
    "Currently, we did not prefect these steps, so here is a short and imperfect implementation of these so we can start working. [link to notebook](./assumptions.ipynb)\n",
    "\n",
    "> We assume that the data is encoded properly and that by mesuring the distance between comments, we can cluster them based on topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "- Agglomerative\n",
    "- K-Means\n",
    "- HDBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Benchmarks\n",
    "- Davies-Bouldin\n",
    "- Silhouette\n",
    "- Calinski-Harabasz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install scikit-learn pandas numpy tqdm python-dotenv google-genai plotly matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks utills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_subdatasets(dataset_path, subsets_amount, subset_size):\n",
    "    \"\"\"\n",
    "    Reads a dataset from a path and splits it into smaller dataframes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_path : str\n",
    "        Path to the dataset file\n",
    "    subsets_amount : int\n",
    "        Number of subdatasets to create\n",
    "    subset_size : int\n",
    "        Size of each subdataset\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with dataset names as keys and pandas DataFrames as values\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not read dataset from {dataset_path}. Error: {str(e)}\")\n",
    "    \n",
    "    total_required_rows = subsets_amount * subset_size\n",
    "    if len(df) < total_required_rows:\n",
    "        raise ValueError(f\"Dataset has {len(df)} rows, but {total_required_rows} rows are required for {subsets_amount} subsets of size {subset_size}\")\n",
    "    \n",
    "    dataset_name = os.path.splitext(os.path.basename(dataset_path))[0]\n",
    "    \n",
    "    subdatasets = {}\n",
    "    for i in range(subsets_amount):\n",
    "        subset = df.sample(n=subset_size, random_state=i)\n",
    "        subset_name = f\"{dataset_name}_subset_{subset_size}_{i}\"\n",
    "        subdatasets[subset_name] = subset\n",
    "    \n",
    "    return subdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "def clustering_evaluation(X, labels):\n",
    "    \n",
    "    if len(set(labels)) > 1 and len(set(labels)) < len(X):\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "        davies_bouldin = davies_bouldin_score(X, labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(X, labels)\n",
    "    else:\n",
    "        silhouette = float('nan')\n",
    "        davies_bouldin = float('nan')\n",
    "        calinski_harabasz = float('nan')\n",
    "    \n",
    "    return silhouette, davies_bouldin, calinski_harabasz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def clustering_benchmark(models_dict, dataframes_dict):    \n",
    "    results = []\n",
    "    total_iterations = len(dataframes_dict) * len(models_dict)\n",
    "\n",
    "    with tqdm(total=total_iterations, desc='Clustering Progress') as pbar:\n",
    "        for dataset_name, df in dataframes_dict.items():\n",
    "            embed_cols = [col for col in df.columns if re.match(r'embed_dim_\\d+', col)]\n",
    "            \n",
    "            for model_name, model in models_dict.items():\n",
    "                model.fit(df[embed_cols])\n",
    "                labels = model.labels_\n",
    "                df['Cluster_Assignment'] = labels\n",
    "                \n",
    "                silhouette, davies_bouldin, calinski_harabasz = clustering_evaluation(df[embed_cols], labels)\n",
    "                \n",
    "                results.append({\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Dataset Size': len(df),\n",
    "                    'Model': model_name,\n",
    "                    'Silhouette Score': silhouette,\n",
    "                    'Davies-Bouldin Index': davies_bouldin,\n",
    "                    'Calinski-Harabasz Index': calinski_harabasz,\n",
    "                    'Number of Clusters': len(set(labels)) if hasattr(model, 'labels_') else 0\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step, lets just put our points on the plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"./datasets/with-assumptions/jack_vs_calley_1000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "if df['encoded'].dtype == 'object' and isinstance(df['encoded'].iloc[0], str):\n",
    "    df['encoded'] = df['encoded'].apply(lambda x: ast.literal_eval(x))\n",
    "    \n",
    "df['x'] = df['encoded'].apply(lambda x: x[0])\n",
    "df['y'] = df['encoded'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_scatter(df):        \n",
    "    fig = px.scatter(\n",
    "        df, \n",
    "        x='x', \n",
    "        y='y',\n",
    "        hover_data=['text'],\n",
    "        title='Visualization of Encoded Points',\n",
    "        labels={'x': 'Dimension 1', 'y': 'Dimension 2'},\n",
    "        opacity=0.7\n",
    "    )\n",
    "\n",
    "    # Improve layout\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        width=900,\n",
    "        height=700\n",
    "    )\n",
    "\n",
    "    # Add grid lines\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not see any pattern at this momnet.\n",
    "To make it more easly ilustrated, lets just pick 50 random comments.\n",
    "Also, we can assume the data is that clusted cuz of the huge dimensionality reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_50_df = df.sample(n=50, random_state=42)\n",
    "plot_scatter(sample_50_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans, HDBSCAN\n",
    "\n",
    "data_path = \"./datasets/with-assumptions/jack_vs_calley_1000.csv\"\n",
    "\n",
    "benchmark_data = create_subdatasets(data_path,20, 50)\n",
    "\n",
    "models_dict = {\n",
    "    \"Agglomerative\": AgglomerativeClustering(n_clusters=5),\n",
    "    \"HDBSCAN\": HDBSCAN(min_cluster_size=3),\n",
    "    \"KMeans\": KMeans(n_clusters=5, random_state=42)\n",
    "}\n",
    "\n",
    "\n",
    "results_df = clustering_benchmark(models_dict, benchmark_data)\n",
    "\n",
    "results_df.groupby('Model').mean(numeric_only=True).style\\\n",
    "    .format('{:.2f}')\\\n",
    "    .set_caption('Clustering Benchmark Results')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file= f\"./benchmarks results/jack_vs_cally_small.csv\"\n",
    "results_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run some clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Clustering Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calinski-Harabasz Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Comparison and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Visualization and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Clustering with LLM Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERTopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
