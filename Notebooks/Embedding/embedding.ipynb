{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Train Sentence Transformer, Find DB ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence embeddings are vectors (lists of numbers) that represent the meaning of entire sentences.\n",
    "\n",
    "Firstly we do prerocessing in [preprocessing.ipynb](../datasets/preprocessing.ipynb) <br>\n",
    "Afterward we create the embedding using a sentence transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Utils - used across the board. <br>\n",
    "2. Sentence Transformers - explanation about why what and how of transformers. <br>\n",
    "3. Refining Transformer - possilbe additions to make the sentence transformer the best. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import emoji\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import data.data_utils as data_utils\n",
    "import model.model_utils as model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"jack_vs_calley_1000\"\n",
    "data = data_utils.get_comments(dataset_name)\n",
    "tweets = data_utils.get_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = model_utils.get_sentence_transformer_model()\n",
    "all_mpnet_base_v2 = SentenceTransformer('all-mpnet-base-v2')\n",
    "all_MiniLM_L6_v2 = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = model_utils.get_sentence_transformer_model()\n",
    "embeddings = st_model.encode(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Default Denoising Stategy**</u> <br>\n",
    "The default noising strategy in DenoisingAutoEncoderDataset (random token deletion with a 30% probability) is a good starting point, but itâ€™s not optimized for tweets. <br>\n",
    "\n",
    "<u>**Best Noising Strategy for Tweets**</u> <br>\n",
    "\n",
    "**Conservative Token Deletion** - Delete tokens with a lower probability than the default (e.g., 10-15% instead of 30%) because tweets are short, and deleting too many tokens can destroy the meaning.\n",
    "Avoid deleting critical tokens like hashtags, mentions, URLs, and emojis to preserve tweet-specific structure. <br>\n",
    "\n",
    "**Token Swapping** - Swap adjacent tokens with a small probability (e.g., 10%) to simulate minor word order variations, which can happen in informal writing. Ensure swaps donâ€™t break hashtags, mentions, or URLs.\n",
    "\n",
    "**Token Replacement (Simulate Typos)** - Replace tokens with similar-looking characters (e.g., \"love\" â†’ \"l0ve\", \"the\" â†’ \"teh\") to mimic common typos in tweets. Use a small probability (e.g., 5-10%) to avoid overwhelming the text with typos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_special_token(token):\n",
    "    if token.startswith(\"#\") or token.startswith(\"@\"):\n",
    "        return True\n",
    "    elif contains_emoji(token):\n",
    "        return True\n",
    "    elif is_url(token):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def contains_emoji(token):\n",
    "    for char in token:\n",
    "        if char in emoji.EMOJI_DATA:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_url(token):\n",
    "    try:\n",
    "        result = urlparse(token)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_delete_conservative(tokens, deletion_prob = 0.1):\n",
    "    noisy_tokens = []\n",
    "    for token in tokens:\n",
    "        if is_special_token(token) or random.random() > deletion_prob:\n",
    "            noisy_tokens.append(token)\n",
    "\n",
    "    return noisy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_swapped(tokens, swap_prob = 0.1):\n",
    "    noisy_tokens = tokens.copy()\n",
    "    for i in range(len(noisy_tokens) - 1):\n",
    "        if random.random() < swap_prob and not is_special_token(noisy_tokens[i]) and not is_special_token(noisy_tokens[i + 1]):\n",
    "            noisy_tokens[i], noisy_tokens[i + 1] = noisy_tokens[i + 1], noisy_tokens[i]\n",
    "\n",
    "    return noisy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to introduce a typo\n",
    "def introduce_typo(word):\n",
    "    if len(word) < 2:\n",
    "        return word\n",
    "\n",
    "    idx = random.randint(0, len(word) - 2)\n",
    "    return word[:idx] + word[idx + 1] + word[idx] + word[idx + 2:]\n",
    "\n",
    "def get_typo_tokens(tokens, typo_prob = 0.05):\n",
    "    noisy_tokens = []\n",
    "    for token in tokens:\n",
    "        if is_special_token(token) or random.random() > typo_prob:\n",
    "            noisy_tokens.append(token)\n",
    "        else:\n",
    "            noisy_tokens.append(introduce_typo(token))\n",
    "\n",
    "    return noisy_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Train Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(tweet, deletion_prob=0.1, swap_prob=0.1, typo_prob=0.05):\n",
    "    tokens = tweet.split()\n",
    "    tokens = get_tokens_delete_conservative(tokens, deletion_prob)\n",
    "    tokens = get_tokens_swapped(tokens, swap_prob)\n",
    "    tokens = get_typo_tokens(tokens, typo_prob)\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Test the noise function\n",
    "tweet = \"I love my new iPhone ðŸ“± #TechLover @Apple https://apple.com\"\n",
    "noisy_tweet = add_noise(tweet)\n",
    "print(f\"Original: {tweet}\")\n",
    "print(f\"Noisy: {noisy_tweet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "for tweet in tweets:\n",
    "    augmented_tweet = add_noise(tweet)\n",
    "    train_examples.append({\"sentence1\": tweet, \"sentence2\": augmented_tweet})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why are Transformers Used for This Task?** <br>\n",
    "<u>Contextual Understanding</u>\n",
    "Transformers excel at understanding the context of words within a sentence. They use an \"attention mechanism\" that allows them to weigh the importance of different words in the sentence when determining its meaning. They can effectively capture long-range dependencies between words in a sentence. This means they can understand how words that are far apart in a sentence relate to each other, which is essential for understanding the overall meaning of the sentence. <br>\n",
    "\n",
    "<u>Pre-training and Fine-tuning</u>\n",
    "Transformer models can be pre-trained on massive amounts of text data, allowing them to learn a deep understanding of language. This pre-trained knowledge can then be fine-tuned for specific tasks, such as generating sentence embeddings. Â <br>\n",
    "\n",
    "<u>Superior Performance</u>\n",
    "Transformer-based models have consistently achieved state-of-the-art results on a wide range of NLP tasks, including sentence embedding. Their ability to capture complex semantic relationships makes them ideal for this task. Â  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer is a deep learning architecture \n",
    "\n",
    "* **Key Features:**\n",
    "    * Attention Mechanism: This is the core of the Transformer, allowing the model to focus on relevant parts of the input.\n",
    "    * Encoder-Decoder Structure: The original Transformer architecture consists of an encoder (to process the input) and a decoder (to generate the output).\n",
    "    * Parallel Processing: Unlike recurrent neural networks (RNNs), Transformers can process input sequences in parallel, leading to faster training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer-architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we give every word a vector based on pre trained \"Embedding Space\". <br>\n",
    "Existing models like GloVe: Global Vectors For Word Representation <br>\n",
    "<img src=\"images/embedding-space.png\" width=\"200\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Positional Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector that gives contex based on position of word in sentence. <br>\n",
    "The positional encodings have the same dimension d<sub>model</sub>\n",
    "as the embeddings, so that the two can be summed. <br>\n",
    "where pos is the position and i is the dimension. <br>\n",
    "<img src=\"images/positional-encoder.png\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How relevant is the i'th word to the other words. <br>\n",
    "For every word we have an attention vector generated which captures contextual relationships between words. <br>\n",
    "<img src=\"images/attention-vectors.png\" width=\"400\" height=\"200\"> <br>\n",
    "\n",
    "Each Word Vector is broken to into 3 same dimentional vectors. <Br>\n",
    "Q - query: what im looking for. <br>\n",
    "K - key: what i can offer. <br>\n",
    "V - value: what i actaully have to offer. <br>\n",
    "\n",
    "How are Q,K,V calculated ? <br>\n",
    "For each of the h attention heads the models stores three weight matrices. <br>\n",
    "W<sub>Q</sub><sup>(i)</sup>: The weight matrix for the Query projection. <br>\n",
    "W<sub>K</sub><sup>(i)</sup>: The weight matrix for the Key projection. <br>\n",
    "W<sub>V</sub><sup>(i)</sup>: The weight matrix for the Value projection. <br>\n",
    "Each of these matrices has dimensions (d<sub>model</sub>, d<sub>k</sub>), where:\n",
    "d<sub>model</sub> is the dimension of the input embeddings (e.g., 768 for BERT base).\n",
    "d<sub>k</sub> is the dimension of the key/query/value vectors for each head (and is typically d<sub>model</sub>/h).\n",
    "Total number of weights: So, for each head, you have d<sub>model</sub> * d<sub>k</sub> weights in each of the three matrices. For h heads, the total number of weights is 3 * h * d<sub>model</sub> * d<sub>k</sub>. Since d<sub>k</sub> is often d<sub>model</sub>/h, this simplifies to 3 * d<sub>model</sub><sup>2</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add And Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes the output matrix from attention, keeps feeding the calculated input embedding. <br>\n",
    "This is done to ensure there is stronger information signal that flows through deep networks. Required because of vanishing gradients problem in back propagation (gradient becomes 0 after many backpropagation). <br>\n",
    "To prevent, we induce stronger signals from the inputs in different parts of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT (Bidirectional Encoder Representations from Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is a specific Transformer-based model developed by Google.\n",
    "* **Key Features:**\n",
    "    * **Encoder-Only:** Uses only the Transformer's encoder.\n",
    "    * **Bidirectional Training:** Trained to understand context from both directions.\n",
    "    * **Pre-training and Fine-tuning:** Pre-trained on a massive dataset, then fine-tuned for specific tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you can derive sentence embeddings from BERT (e.g., by averaging word embeddings), the resulting embeddings are often not optimal for tasks that require comparing the semantic similarity of sentences. <br>\n",
    "\n",
    "Sentence Transformers are a framework or a methodology. They don't dictate a single architecture. Instead, they provide a way to train Transformer models to generate high-quality sentence embeddings. Â  \n",
    "You can use various base Transformer architectures (like BERT, RoBERTa, MPNet, etc.) within the Sentence Transformers framework. <br>\n",
    "\n",
    "Input Sentence --> Tokenization --> Transformer Encoder --> Pooling --> Sentence Embedding <br>\n",
    "\n",
    "\"all-mpnet-base-v2\" <br>\n",
    "When you see a model like \"all-mpnet-base-v2\", that means that the MPNet base Transformer architecture was used, and then it was fine tuned using the sentence transformer methodology. Â  \n",
    "Therefore, it is a sentence transformer, that uses the mpnet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't directly compare the embedding of two sentences if they have different numbers of tokens.\n",
    "You need a way to summarize the values of each sentence into a single, consistent representation.\n",
    "\n",
    "The pooling layer produces a single set of numbers that represents the \"summary\" of the sentence.\n",
    "This set of numbers is the sentence embedding.\n",
    "Because the pooling layer always produces a fixed-size output, you can now easily compare the sentence embeddings of different sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simalrity By Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_index_to_compare = 13\n",
    "similarity_scores = cosine_similarity(embeddings, embeddings)[text_index_to_compare]\n",
    "similarity_df = pd.DataFrame({'similarity': similarity_scores, 'sentence': data})\n",
    "similarity_df = similarity_df.sort_values('similarity', ascending=False)\n",
    "display(similarity_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/preprocessing.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Transformers handle preprocessing in a specific way that's optimized for creating effective sentence embeddings. <br>\n",
    "\n",
    "**Tokenization:** <br>\n",
    "typically use sub-word tokenization algorithms like WordPiece or Byte-Pair Encoding (BPE). These algorithms break words into smaller, more frequent units, which helps handle out-of-vocabulary words and reduces the vocabulary size. <br>\n",
    "\n",
    "**Lowercasing (Optional):** <br>\n",
    "Some Sentence Transformer models might apply lowercasing as a preprocessing step. However, this is not always the case. <br>\n",
    "\n",
    "**No Stemming or Lemmatization:** <br>\n",
    "Sentence Transformers generally do not perform stemming or lemmatization.\n",
    "The reason is that these techniques can sometimes lose semantic information, which is crucial for generating accurate sentence embeddings.\n",
    "\n",
    "**No Stop Word Removal:** <br>\n",
    "Sentence Transformers also typically do not remove stop words.\n",
    "The context provided by stop words can be important for understanding the meaning of a sentence. <br>\n",
    "\n",
    "**Normalization:** <br>\n",
    "Sentence Transformers do perform normalization in the sense that they convert the input text into a numerical representation (embeddings).\n",
    "They also normalize the embeddings themselves, so that they have unit length. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting text into smaller units called tokens. These tokens can be words, subwords, or even characters.\n",
    "\n",
    "**Special Tokens** <br>\n",
    "[CLS] -> Added at the beginning of each input sequence and is used for classification tasks (`<s>`). <br>\n",
    "[SEP] -> Separate two sentences in a sequence (`</s>`). <br>\n",
    "[UNK] -> When a word is not found in the vocabulary, it's replaced with this. <br>\n",
    "`##` - this token is together with the last token\n",
    "\n",
    "**Padding** <br>\n",
    "Transformers require input sequences to have a fixed length. <br>\n",
    "The shorter sentences are padded with a special token, usually [PAD]. <br>\n",
    "If some texts are longer than the length that the model can handl the texts are either truncated (cut off at the maximum length) or split into multiple segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Numbers:** <br>\n",
    "Sentence Transformers might keep numbers as tokens, as they can sometimes carry semantic meaning.\n",
    "If you want to remove them, you would need to do it as a separate preprocessing step before feeding the text to the Sentence Transformer.\n",
    "\n",
    "**Remove Punctuation:** <br>\n",
    "Sentence Transformers' tokenizers often handle punctuation marks as separate tokens.\n",
    "They might keep some punctuation, as it can contribute to meaning (e.g., question marks, exclamation points).\n",
    "If you want to remove all punctuation, you would have to do it beforehand.\n",
    "\n",
    "**Remove Special Characters and Symbols:** <br>\n",
    "Similar to punctuation, Sentence Transformers might keep some special characters and symbols.\n",
    "Again, you would need to remove them manually if desired.\n",
    "\n",
    "**Remove Non-English Words:** <br>\n",
    "Sentence Transformers are typically trained on multilingual or English corpora.\n",
    "They might not explicitly remove non-English words, but they might not produce meaningful embeddings for them.\n",
    "If you want to filter out non-English words, you would need to do it beforehand.\n",
    "\n",
    "**Remove Words with Less Than Three Letters:** <br>\n",
    "Sentence Transformers generally do not filter out short words.\n",
    "You would need to do this manually if you want to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = st_model.tokenizer\n",
    "tokenized_texts = []\n",
    "for text in data:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokenized_texts.append(tokens)\n",
    "\n",
    "# Print the tokenized texts\n",
    "for tokens in tokenized_texts:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tokenization Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(\"This is f* **ed and crazyyyy\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"It was a great discussion\",\n",
    "    \"Calley comes off shady AF.\",\n",
    "    \"Total Jack count anyone?\",\n",
    "    \"Wow is Calley tongue tied.\"\n",
    "]\n",
    "batch = tokenizer(sentences, padding=True)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in batch[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "determines the word base form, or lemma. It considers the context of the word and aims to produce actual words from the dictionary.   \n",
    "Example:\n",
    "Original words: \"better,\" \"good,\" \"best\"\n",
    "Lemmatized words: \"good,\" \"good,\" \"good\"\n",
    "\n",
    "Source: A systematic review of text stemming techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common words in a language that are often filtered out of text analysis tasks because they are considered to carry little semantic meaning or contribute minimally to the overall understanding of a text.\n",
    "\n",
    "Examples of common English stop words include:\n",
    "Articles: the, a, an\n",
    "Prepositions: in, on, at, with, for\n",
    "Conjunctions: and, or, but\n",
    "Pronouns: I, you, he, she, it, they, we\n",
    "Auxiliary Verbs: is, am, are, was, were, will, shall, can, could, may, might, must\n",
    "\n",
    "**Why remove stop words?**\n",
    "Reduced Dimensionality: By removing stop words, we can reduce the dimensionality of the text data, making it easier to process and analyze.\n",
    "Improved Performance: Removing stop words can improve the performance of many NLP tasks, such as text classification, sentiment analysis, and information retrieval.\n",
    "Focus on Key Words: By filtering out stop words, we can focus on the most important words in the text, which can lead to more accurate and meaningful analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reducing words to their root form by removing suffixes, prefixes, or other affixes. \n",
    "Example:\n",
    "Original words: \"cats,\" \"catlike,\" \"catty\"   \n",
    "Stemmed words: \"cat\", \"catlik\", \"catti\"   \n",
    "\n",
    "Source: Kaur, J.; Buttar, P.K. A systematic review on stopword removal algorithms. Int. J. Future Revolut. Comput. Sci. Commun. Eng. 2018, 4, 207â€“210."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = [len(sentence.split()) for sentence in data]\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.hist(word_counts, bins=200, alpha=0.7, label='sentences')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Frequency of number of words per sentence')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_512 = sum(1 for count in word_counts if count > 512)\n",
    "over_256 = sum(1 for count in word_counts if count > 256)\n",
    "\n",
    "print(f\"Number of sentences with over 512 words: {over_512}\")\n",
    "print(f\"Number of sentences with over 256 words: {over_256}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Vocabulary ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = st_model.tokenizer\n",
    "tokenized_texts = []\n",
    "for text in data:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokenized_texts.append(tokens)\n",
    "\n",
    "# Print the tokenized texts\n",
    "for tokens in tokenized_texts:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see some words are not in the vocabulary. utube (youtube), bitcoiner, doesnt understand the name kally, psychpath, behaving .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training A Model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Training Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| **Method**                          | **How It Works**                                                                 | **Pros**                                                                 | **Cons**                                                                 |\n",
    "|-------------------------------------|----------------------------------------------------------------------------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|\n",
    "| **Standard Supervised Fine-Tuning** | Trains on labeled data with a task-specific loss (e.g., cosine similarity).     | High accuracy, task-specific optimization.                               | Requires labeled data, limited by dataset size/quality.                  |\n",
    "| **Unsupervised Fine-Tuning**        | Uses unlabeled data with objectives like TSDAE (denoising) or SimCSE (contrastive). | No labeled data needed, adapts to domain.                                | Lower accuracy than supervised, depends on data quality.                 |\n",
    "| **Self-Supervised (MLM)**           | Fine-tunes on unlabeled data by predicting masked words.                        | No labeled data needed, improves contextual understanding.               | Not task-specific, needs large data, requires further adaptation.        |\n",
    "| **SetFit (Few-Shot Learning)**      | Trains on small labeled data with contrastive learning and an SVM classifier.   | Very efficient, needs little labeled data, fast training.                | Requires some labeled data, performance tied to dataset quality.         |\n",
    "| **Zero-Shot Learning (LLM)**        | Uses an LLM to generate embeddings or classify via prompts, no fine-tuning.     | No training data needed, flexible, leverages LLM knowledge.              | Unpredictable performance, computationally expensive.                    |\n",
    "| **Contrastive Learning (Hard Negatives)** | Trains with positive/negative pairs, often mining hard negatives (InfoNCE loss). | Improves fine-grained similarity, can be supervised or unsupervised.     | Requires careful pair selection, can be computationally intensive.       |\n",
    "| **Knowledge Distillation**          | A teacher model (e.g., a larger bi-encoder) trains a smaller Sentence Transformer. | Transfers knowledge from a better model, efficient inference.            | Needs a trained teacher model, may lose some accuracy.                   |\n",
    "| **Multi-Task Learning**             | Trains on multiple tasks (e.g., similarity, classification) simultaneously.     | Better generalization, leverages diverse data.                           | More complex to implement, requires diverse labeled data.                |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contrastive Tension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised method that uses two models. If the same sentences are passed to Model1 and Model2, then the respective sentence embeddings should get a large dot-score. If the different sentences are passed, then the sentence embeddings should get a low score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimCSE (Simple Contrastive Learning of Sentence Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SimCSE Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimCSE leverages contrastive learning. It takes a sentence and creates two slightly different versions of it, treating them as positive pairs. Â  \n",
    "The core idea is to maximize the similarity between these positive pairs while minimizing the similarity between all other sentences (negative pairs) in the batch. Â  \n",
    "The most common form of SimCSE uses dropout as the only form of data augmentation. Applying dropout twice to the same sentence yields two slightly different embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shorter Training\n",
    "Â  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training initially looked like it was going to take 30 hours. so we took several steps in order to shorten it. <br>\n",
    "\n",
    "If GPU:\n",
    "* move model to cuda\n",
    "* use amp (float 32 to 16)\n",
    "\n",
    "CPU Meaasures: \n",
    "1. **Gradient Accumalation** - Instead of processing the entire batch at once, the batch is divided into smaller \"micro-batches\". The model performs a forward pass and a backward pass on each micro-batch.\n",
    "The gradients are calculated for each micro-batch but are not immediately applied to update the model's weights. They are accumulated (added together). And After processing they are used to update the model's weights.\n",
    "\n",
    "2. **Use Smaller Model** - MiniLM-L6-v2 instead of all-mpnet-base-v2\n",
    "\n",
    "3. **Use Faster Optimizer** - AdamW instead of Adam\n",
    "\n",
    "Other Considerations: num_workers not needed (data is small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = all_MiniLM_L6_v2\n",
    "model_dir = os.path.join(model_utils.models_dir, \"trained_miniLM_twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(train_examples)\n",
    "\n",
    "train_loss = losses.MultipleNegativesRankingLoss(st_model)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\", \n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    fp16=False,  # Disable mixed precision (not supported on CPU)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=train_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TSDAE (Transformer-based Denoising Autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem - Most models are encoder only and do not support decoder**\n",
    "\n",
    "If your data is significantly different from the general text that sentence transformer was trained on, TSDAE could help improve the model's performance. <br>\n",
    "\n",
    "First, Corrupt a sentence in some way (e.g., randomly masking words, deleting words, or shuffling words). Than, we train the model to minimize the difference between the reconstructed sentence and the original sentence. <br>\n",
    "\n",
    "<u>AutoEncoder</u> <br>\n",
    "Neural network that learns to copy its input to its output. <br>\n",
    "Encoder Compresses the input into a lower-dimensional representation (latent space). Â  <br>\n",
    "Decoder Reconstructs the original input from the latent representation. Â  <br>\n",
    "The idea is that by forcing the network to compress and then reconstruct the input, it learns to capture the most important features of the data.\n",
    "\n",
    "<u>Denoising AutoEncoder</u> <br>\n",
    "A variation of an autoencoder where the input is corrupted or \"noised\" before being fed into the encoder. Â <br> \n",
    "The decoder is then tasked with reconstructing the original, uncorrupted input. Â  <br>\n",
    "This forces the network to learn more robust and generalizable representations, as it has to learn to \"denoise\" the input. Â  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = \"AIzaSyDFm56mSyyYDUAL8yeWlYJ3Rf9z_fNFU9A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "result = genai.embed_content(\n",
    "        model=\"models/text-embedding-004\",\n",
    "        content=\"What is the meaning of life?\")\n",
    "\n",
    "print(str(result['embedding']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Transformer Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Our Task Short Text Clustring - Semantic similarity is the most relavant\n",
    "\n",
    "| Evaluator                     | Task Focus                     | Dataset Example         | Metric                     | Relevance to Short Text Clustering |\n",
    "|-------------------------------|--------------------------------|-------------------------|----------------------------|-----------------------------|\n",
    "| **EmbeddingSimilarityEvaluator** | Semantic similarity           | STS-B                   | Spearman/Pearson correlation | **High**: Directly assesses embedding quality for similarity, crucial for clustering. |\n",
    "| **BinaryClassificationEvaluator** | Binary classification         | MRPC, NLI (binarized)   | Accuracy, F1, AP           | **Moderate**: Can test if embeddings distinguish similar/dissimilar pairs, indirectly useful. |\n",
    "| **TripletEvaluator**          | Triplet ranking               | ALLNLI                  | Triplet accuracy           | **Moderate**: Can improve embeddings for clustering by enforcing distance margins. |\n",
    "| **LabelAccuracyEvaluator**    | Classification                | Any labeled dataset     | Classification accuracy    | **Low**: Focuses on classification, not clustering. |\n",
    "| **MSEEvaluator**              | Regression                    | STS-B                   | Mean squared error         | **Low**: Regression-focused, less interpretable for clustering. |\n",
    "| **ParaphraseMiningEvaluator** | Paraphrase detection          | Quora Question Pairs    | Precision, Recall, F1      | **Moderate**: Useful if clustering involves grouping paraphrases. |\n",
    "| **InformationRetrievalEvaluator** | Information retrieval       | MS MARCO, BEIR          | NDCG, MRR, Recall@k        | **Low**: Focuses on ranking, not clustering. |\n",
    "| **TranslationEvaluator**      | Cross-lingual alignment       | Tatoeba                 | Spearman/Pearson correlation | **Low**: Only relevant for cross-lingual clustering. |\n",
    "| **RerankingEvaluator**        | Reranking                     | MS MARCO                | Ranking accuracy           | **Low**: Focuses on ordering, not clustering. |\n",
    "| **SequentialEvaluator**       | Multiple evaluations          | Depends on evaluators   | Combined metrics           | **High (if used with relevant evaluators)**: Combines multiple relevant evaluations. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STS (Semantic Textual Similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STS benchmarks typically involve pairs of sentences and human-annotated scores that indicate their similarity. Â  \n",
    "Models are evaluated by comparing their predicted similarity scores to these human-generated scores.\n",
    "Common evaluation metrics include Pearson's correlation and Spearman's rank correlation. Â  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
    "\n",
    "# Initialize the evaluator\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=eval_dataset[\"sentence1\"],\n",
    "    sentences2=eval_dataset[\"sentence2\"],\n",
    "    scores=eval_dataset[\"score\"],\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    "    name=\"sts-dev\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_mpnet_base_v2 = dev_evaluator(all_mpnet_base_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_mpnet_base_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_MiniLM_L6_v2 = dev_evaluator(all_MiniLM_L6_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_MiniLM_L6_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### all-mpnet-base-v2 trained SimCSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERTopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
