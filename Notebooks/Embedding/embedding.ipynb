{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizaiton\n",
    "splitting text into smaller units called tokens. These tokens can be words, subwords, or even characters.\n",
    "\n",
    "#### Special Tokens\n",
    "[CLS] -> Added at the beginning of each input sequence and is used for classification tasks (`<s>`). <br>\n",
    "[SEP] -> Separate two sentences in a sequence (`</s>`). <br>\n",
    "[UNK] -> When a word is not found in the vocabulary, it's replaced with this. <br>\n",
    "`##` - this token is together with the last token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(\"This is f* **ed and crazyyyy\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"It was a great discussion\",\n",
    "    \"Calley comes off shady AF.\",\n",
    "    \"Total Jack count anyone?\",\n",
    "    \"Wow is Calley tongue tied.\"\n",
    "]\n",
    "batch = tokenizer(sentences, padding=True)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in batch[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transofrmer Architecture\n",
    "<img src=\"images/transformer-architecture.png\">\n",
    "\n",
    "#### Inputs\n",
    "Input Embedding + Positional Encoding. \n",
    "\n",
    "**Padding** <br>\n",
    "Transformers require input sequences to have a fixed length. <br>\n",
    "The shorter sentences are padded with a special token, usually [PAD]. <br>\n",
    "If some texts are longer than the length that the model can handl the texts are either truncated (cut off at the maximum length) or split into multiple segments.\n",
    "\n",
    "**Input Embedding** <br>\n",
    "First we give every word a vector based on pre trained \"Embedding Space\". <br>\n",
    "Existing models like GloVe: Global Vectors For Word Representation <br>\n",
    "<img src=\"images/embedding-space.png\" width=\"200\" height=\"200\">\n",
    "\n",
    "**Positional Encoder (PE)** <br>\n",
    "Vector that gives contex based on position of word in sentence. <br>\n",
    "The positional encodings have the same dimension d<sub>model</sub>\n",
    "as the embeddings, so that the two can be summed. <br>\n",
    "where pos is the position and i is the dimension. <br>\n",
    "<img src=\"images/positional-encoder.png\" width=\"300\" height=\"200\">\n",
    "\n",
    "#### Encoding Block\n",
    "**Multi-Head Attention** <br>\n",
    "How relevant is the i'th word to the other words. <br>\n",
    "For every word we have an attention vector generated which captures contextual relationships between words. <br>\n",
    "<img src=\"images/attention-vectors.png\" width=\"400\" height=\"200\"> <br>\n",
    "\n",
    "##### Attention Nerdy details\n",
    "Each Word Vector is broken to into 3 same dimentional vectors. <Br>\n",
    "Q - query: what im looking for. <br>\n",
    "K - key: what i can offer. <br>\n",
    "V - value: what i actaully have to offer. <br>\n",
    "\n",
    "How are Q,K,V calculated ? <br>\n",
    "For each of the h attention heads the models stores three weight matrices. <br>\n",
    "W<sub>Q</sub><sup>(i)</sup>: The weight matrix for the Query projection. <br>\n",
    "W<sub>K</sub><sup>(i)</sup>: The weight matrix for the Key projection. <br>\n",
    "W<sub>V</sub><sup>(i)</sup>: The weight matrix for the Value projection. <br>\n",
    "Each of these matrices has dimensions (d<sub>model</sub>, d<sub>k</sub>), where:\n",
    "d<sub>model</sub> is the dimension of the input embeddings (e.g., 768 for BERT base).\n",
    "d<sub>k</sub> is the dimension of the key/query/value vectors for each head (and is typically d<sub>model</sub>/h).\n",
    "Total number of weights: So, for each head, you have d<sub>model</sub> * d<sub>k</sub> weights in each of the three matrices. For h heads, the total number of weights is 3 * h * d<sub>model</sub> * d<sub>k</sub>. Since d<sub>k</sub> is often d<sub>model</sub>/h, this simplifies to 3 * d<sub>model</sub><sup>2</sup>.\n",
    "\n",
    "**TODO** Continue\n",
    "\n",
    "##### Add & Norm\n",
    "Takes the output matrix from attention, keeps feeding the calculated input embedding. <br>\n",
    "This is done to ensure there is stronger information signal that flows through deep networks. Required because of vanishing gradients problem in back propagation (gradient becomes 0 after many backpropagation). <br>\n",
    "To prevent, we induce stronger signals from the inputs in different parts of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "<img src=\"images/preprocessing.png\">\n",
    "\n",
    "##### Tokenization\n",
    "WordPiece for BERT\n",
    "1. Remove numbers (2, 1 . . . ). \n",
    "2. Remove punctuation marks (‘!’, ’, -, ”, :, ?, [], \\, . . . ).\n",
    "3. Remove special characters (~, @, #, $, %, &, =, +). \n",
    "4. Remove symbols (e.g., ). \n",
    "5. Remove non-English words, such as اسم. \n",
    "6. Remove words with less than three letters. \n",
    "\n",
    "\n",
    "##### Lemmatization\n",
    "determines the word base form, or lemma. It considers the context of the word and aims to produce actual words from the dictionary.   \n",
    "Example:\n",
    "Original words: \"better,\" \"good,\" \"best\"\n",
    "Lemmatized words: \"good,\" \"good,\" \"good\"\n",
    "\n",
    "Source: A systematic review of text stemming techniques.\n",
    "\n",
    "##### Stop words\n",
    "common words in a language that are often filtered out of text analysis tasks because they are considered to carry little semantic meaning or contribute minimally to the overall understanding of a text.\n",
    "\n",
    "Examples of common English stop words include:\n",
    "Articles: the, a, an\n",
    "Prepositions: in, on, at, with, for\n",
    "Conjunctions: and, or, but\n",
    "Pronouns: I, you, he, she, it, they, we\n",
    "Auxiliary Verbs: is, am, are, was, were, will, shall, can, could, may, might, must\n",
    "\n",
    "**Why remove stop words?**\n",
    "Reduced Dimensionality: By removing stop words, we can reduce the dimensionality of the text data, making it easier to process and analyze.\n",
    "Improved Performance: Removing stop words can improve the performance of many NLP tasks, such as text classification, sentiment analysis, and information retrieval.\n",
    "Focus on Key Words: By filtering out stop words, we can focus on the most important words in the text, which can lead to more accurate and meaningful analysis.\n",
    "\n",
    "##### Stemming\n",
    "reducing words to their root form by removing suffixes, prefixes, or other affixes. \n",
    "Example:\n",
    "Original words: \"cats,\" \"catlike,\" \"catty\"   \n",
    "Stemmed words: \"cat\", \"catlik\", \"catti\"   \n",
    "\n",
    "Source: Kaur, J.; Buttar, P.K. A systematic review on stopword removal algorithms. Int. J. Future Revolut. Comput. Sci. Commun. Eng. 2018, 4, 207–210.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can change the length of the vector from 512?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = \"AIzaSyDFm56mSyyYDUAL8yeWlYJ3Rf9z_fNFU9A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "result = genai.embed_content(\n",
    "        model=\"models/text-embedding-004\",\n",
    "        content=\"What is the meaning of life?\")\n",
    "\n",
    "print(str(result['embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datasetName = \"jack_vs_calley_1000\" \n",
    "\n",
    "# Load the comments dataset\n",
    "df = pd.read_csv(f\"../datasets/youtube-comments/{datasetName}.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"../datasets/youtube-comments/{datasetName}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "offensive_words = [\"fuck\", \"fucked\", \"fucking\", \"shit\", \"bitch\", \"cunt\", \"ass\", \"damn\", \"hell\"]  # Example\n",
    "\n",
    "def guess_uncensored_word(censored_word):\n",
    "    prefix, asterisks, suffix = re.match(r\"([a-zA-Z]*)(\\*+)([a-zA-Z]*)\", censored_word, re.IGNORECASE).groups() if re.match(r\"([a-zA-Z]*)(\\*+)([a-zA-Z]*)\", censored_word, re.IGNORECASE) else (None, None, None)\n",
    "    if not prefix and not asterisks and not suffix:\n",
    "        return censored_word\n",
    "\n",
    "    for word in offensive_words:\n",
    "        if word.startswith(prefix) and word.endswith(suffix) and len(word) == len(prefix) + len(asterisks) + len(suffix):\n",
    "            return word\n",
    "\n",
    "    return censored_word\n",
    "\n",
    "def uncensor(text):\n",
    "    words = text.split()\n",
    "    uncensored_words = []\n",
    "    for word in words:\n",
    "        if re.search(r\"\\*\", word):\n",
    "            guessed_word = guess_uncensored_word(word)\n",
    "            uncensored_words.append(guessed_word)\n",
    "        else:\n",
    "            uncensored_words.append(word)\n",
    "    return \" \".join(uncensored_words)\n",
    "\n",
    "text = \"This is a f***ing test, with some sh\\*t and a b\\*\\*ch\"\n",
    "uncensored_text = uncensor(text)\n",
    "print(uncensored_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer = SentenceTransformer('all-mpnet-base-v2')\n",
    "embeddings = sentence_transformer.encode(comments)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERTopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
