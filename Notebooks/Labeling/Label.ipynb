{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö BERTopic Labeling Comparison Framework\n",
    "\n",
    "This notebook compares different methods for generating topic labels after clustering YouTube comments using BERTopic.\n",
    "\n",
    "We will compare:\n",
    "- BERTopic built-in labeling (TF-IDF based)\n",
    "- KeyBERT keyword extraction\n",
    "- Gemini (LLM) based labeling\n",
    "- Compute pairwise Jaccard similarity\n",
    "- Visualize and interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping keras as it is not installed.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping keras-nightly as it is not installed.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping keras-preprocessing as it is not installed.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping tf-keras as it is not installed.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping tensorflow as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Collecting transformers==4.36.2\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "Collecting tokenizers==0.13.3\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Collecting sentence-transformers==2.2.2\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Cannot install tokenizers==0.13.3 and transformers==4.36.2 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting bertopic==0.15.0\n",
      "  Using cached bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
      "Collecting keybert==0.7.0\n",
      "  Using cached keybert-0.7.0.tar.gz (21 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ilai\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ilai\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ilai\\anaconda3\\lib\\site-packages (3.9.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (0.29.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (2021.8.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (4.62.3)\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of keras to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested tokenizers==0.13.3\n",
      "    transformers 4.36.2 depends on tokenizers<0.19 and >=0.14\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\ilai\\anaconda3\\lib\\site-packages (0.8.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\ilai\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\n",
      "ERROR: No matching distribution found for itertools\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip uninstall -y keras keras-nightly keras-preprocessing tf-keras tensorflow\n",
    "%pip install keras==2.11.0 tf-keras transformers==4.36.2 tokenizers==0.13.3 sentence-transformers==2.2.2 bertopic==0.15.0 keybert==0.7.0 scikit-learn pandas matplotlib\n",
    "%pip install google-generativeai seaborn itertools numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "import matplotlib.pyplot as plt\n",
    "import google.generativeai as genai\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load CSV\n",
    "df = pd.read_csv(\"..\\data\\youtube_comments\\jack_vs_calley_1000.csv\") \n",
    "texts = df[\"text\"].dropna().astype(str).tolist() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: BERTopic Clustering ---\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "topic_model = BERTopic(embedding_model=embedding_model)\n",
    "topics, probs = topic_model.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyDFm56mSyyYDUAL8yeWlYJ3Rf9z_fNFU9A\")\n",
    "\n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Custom Labelers ---\n",
    "class TopicLabeler:\n",
    "    def __init__(self, texts, topics):\n",
    "        self.texts = texts\n",
    "        self.topics = topics\n",
    "\n",
    "    def label_with_bertopic(self, topic_model):\n",
    "        return {topic: [word for word, _ in topic_model.get_topic(topic)] for topic in set(self.topics) if topic != -1}\n",
    "\n",
    "    def label_with_keybert(self, embedding_model, top_n=5):\n",
    "        kw_model = KeyBERT(model=embedding_model)\n",
    "        labels = {}\n",
    "        for topic in set(self.topics):\n",
    "            if topic == -1:\n",
    "                continue\n",
    "            docs_in_topic = [text for text, t in zip(self.texts, self.topics) if t == topic]\n",
    "            keywords = kw_model.extract_keywords(\" \".join(docs_in_topic), top_n=top_n, stop_words='english')\n",
    "            labels[topic] = [kw[0] for kw in keywords]\n",
    "        return labels\n",
    "    \n",
    "    def label_with_gemini(self, model, max_words=5):\n",
    "        labels = {}\n",
    "        for topic in set(self.topics):\n",
    "            if topic == -1:\n",
    "                continue\n",
    "\n",
    "            # Collect topic texts\n",
    "            docs_in_topic = [text for text, t in zip(self.texts, self.topics) if t == topic]\n",
    "\n",
    "            # Skip small topics\n",
    "            if len(docs_in_topic) < 3:\n",
    "                continue\n",
    "\n",
    "            # Limit number of comments\n",
    "            docs_in_topic = docs_in_topic[:5]\n",
    "\n",
    "            # Limit each comment length (max 300 characters per comment)\n",
    "            docs_in_topic = [text[:300] for text in docs_in_topic]\n",
    "\n",
    "            # Prepare the prompt text\n",
    "            docs_text = \"\\n\".join(docs_in_topic)\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            You are given a group of YouTube comments that share a common topic.\n",
    "            Provide up to {max_words} keywords or short phrases that best summarize the main topic of these comments.\n",
    "            Comments:\n",
    "            {docs_text}\n",
    "            Return the keywords separated by commas only.\n",
    "            \"\"\"\n",
    "\n",
    "            chat = model.start_chat()\n",
    "            response = chat.send_message(prompt)\n",
    "            keywords = response.text.strip().split(',')\n",
    "\n",
    "            labels[topic] = [kw.strip() for kw in keywords]\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Labelers and Generate Topic Labels\n",
    "\n",
    "In this step, we will generate the actual **labels** (keywords) for each topic identified by BERTopic.\n",
    "\n",
    "Each labeler takes the set of documents (comments) in each topic and outputs a list of keywords that are supposed to best describe the topic.\n",
    "\n",
    "### The Labelers:\n",
    "\n",
    "1. **BERTopic Built-in Labeler**  \n",
    "   - This is the default labeler provided by BERTopic.\n",
    "   - It uses TF-IDF to extract the most informative words within each topic cluster.\n",
    "   - Advantage: Simple, fast, and based on word frequency.\n",
    "   - Limitation: Might prioritize frequent but less meaningful words.\n",
    "\n",
    "2. **KeyBERT-based Labeler**  \n",
    "   - Uses the `KeyBERT` library to extract keywords by calculating semantic similarity between words and the overall topic embedding.\n",
    "   - Advantage: Leverages semantic information, can extract less frequent but semantically important keywords.\n",
    "   - Limitation: May produce more specific labels that are harder to generalize.\n",
    "\n",
    "3. **Gemini-based Labeler (LLM)**  \n",
    "   - Uses Google's Gemini (Generative AI) to generate keywords by prompting an LLM directly with the topic's documents.\n",
    "   - Advantage: Capable of generating human-like and context-aware labels that may capture abstract concepts.\n",
    "   - Limitation: Computationally expensive and may be influenced by prompt design.\n",
    "\n",
    "---\n",
    "\n",
    "### What Are We Comparing?\n",
    "\n",
    "We aim to compare:\n",
    "- How similar the labels produced by each model are.\n",
    "- Whether the models consistently agree on the most important words for a topic.\n",
    "- Which method generates more meaningful, diverse, and human-friendly topic descriptions.\n",
    "\n",
    "We will do this by:\n",
    "1. Generating labels with all three models.\n",
    "2. Computing **Jaccard Similarity** between every pair of models.\n",
    "3. Visualizing and analyzing the similarity results.\n",
    "4. Manually exploring topics and their generated labels.\n",
    "\n",
    "This step is crucial to understand which labeling method is most suitable for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Labeling ---\n",
    "labeler = TopicLabeler(texts, topics)\n",
    "bertopic_labels = labeler.label_with_bertopic(topic_model)\n",
    "keybert_labels = labeler.label_with_keybert(embedding_model)\n",
    "gemini_labels = labeler.label_with_gemini(gemini_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection on Similarity Analysis\n",
    "\n",
    "While we initially focused on comparing the labelers using **Jaccard Similarity** between keyword sets, this approach alone did not provide sufficient insights into the actual performance or quality of the labelers.\n",
    "\n",
    "### Why?\n",
    "\n",
    "- Jaccard Similarity only measures **overlap** between the generated keywords, but it does not tell us:\n",
    "    - How many topics were successfully labeled.\n",
    "    - How diverse or informative the labels were.\n",
    "    - Whether the models tend to generate long or short labels.\n",
    "    - The uniqueness and variability across topics.\n",
    "\n",
    "- In our case, we observed:\n",
    "    - Very low Jaccard scores across most model pairs.\n",
    "    - Inconsistent patterns that did not lead to clear conclusions.\n",
    "    - For example, Gemini consistently showed low overlap, but this did not necessarily mean it produced poor labels.\n",
    "\n",
    "---\n",
    "\n",
    "## The Motivation for Model Metrics\n",
    "\n",
    "To overcome the limitations of relying on Jaccard Similarity alone, we introduced additional metrics via the `model_metrics()` function:\n",
    "\n",
    "### Added Metrics:\n",
    "- **Effective Coverage** ‚Äî how many topics received a sufficient number of keywords.\n",
    "- **Average Label Length** ‚Äî are the generated labels short and clear or long and verbose?\n",
    "- **Unique Keywords** ‚Äî how many distinct keywords does the model generate across all topics?\n",
    "\n",
    "These metrics allow us to:\n",
    "1. Better characterize each labeler.\n",
    "2. Identify trends beyond simple keyword overlap.\n",
    "3. Make a more informed decision when selecting a labeler for our task.\n",
    "\n",
    "> In real-world applications, a single similarity score is rarely enough.  \n",
    "> Multiple complementary metrics are needed to properly evaluate and select models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\"BERTopic\": bertopic_labels, \"KeyBERT\": keybert_labels, \"Gemini\": gemini_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitation: The Challenge of Quantifying Labeling Quality\n",
    "\n",
    "While we computed several numerical metrics such as Jaccard Similarity, Coverage, Average Label Length, and Keyword Diversity, it is important to acknowledge that **measuring the quality of topic labels is inherently challenging**.\n",
    "\n",
    "Why?\n",
    "- These metrics capture certain aspects like overlap, variety, and quantity, but they do not fully capture:\n",
    "    - The relevance of the labels.\n",
    "    - The interpretability and usefulness of the labels for humans.\n",
    "    - The semantic adequacy of the labels.\n",
    "\n",
    "In practice, selecting the most suitable labeling approach often requires **human judgment**, as numerical metrics alone may not reflect how well the labels truly describe the topics.\n",
    "\n",
    "Therefore, this analysis should be seen as a **preliminary quantitative evaluation**, which ideally should be complemented with a **qualitative (manual) inspection** of selected topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Topic 8 ===\n",
      "\n",
      "--- BERTopic Labels ---\n",
      "the, and, to, of, dr, in, is, he, this, who\n",
      "\n",
      "--- KeyBERT Labels ---\n",
      "calley, calleys, calleyi, healthcare, medicine\n",
      "\n",
      "--- Gemini Labels ---\n",
      "Calley, Big Pharma, Ozempic, Dr. Jack, podcast\n",
      "\n",
      "--- All Texts in Topic 8 ---\n",
      "1. Calley how many more babies and parents have to die while youre busy with niceties? Thats the issue\n",
      "2. I do credit Calle and his sister for opening the dark can of worms about the BigPharma and \"Health Care\" agenda over Ozempic, for one thing.\n",
      "3. This podcast has to be one of the most spectacular events that ever happened in the world. All three persons and the host reached an epic monumental milestone moment at [158:22] minutes into the show. The fact that Calley was able to gather the grit to stay on with the challenging confrontation before him and have Jack and Mary segue into posturing the Jack Medical Bukele act into our governmental framework leveraging the use of state law, and avoiding the clash of Federal Law, is the best news yet. You arrived at a point where \"we the people\" can help you take action immediately to implement the change that this country and the entire world needs to shift towards. It is doable, practical, and easy to present to the people, who are yearning for a solution to this medical JD Rock-feller, plague of a lifetime, perpetrated through lies and disguise upon humankind. How can we help formulate how we all might address our Governers to usher in this online solution you have arrived at, before our v very eyes, and ears. Superb, stupendous, spectacular.\n",
      "4. My question is why did they have him on and not hood sister the Dr ??    I love Dr. Jack and hood no BS talk.He is no match for these two Dr‚Äôs\n",
      "5. Jack is very hard in his questioning when talking about Calleys Mothers death, however, what I will add is that it is easy for Calley to separate himself from the pain and suffering from all the people that have died at the hands of big Pharma\n",
      "6. Haven‚Äôt watched Jack before so can‚Äôt speak to his platform. The things he advocates for in a cursory search aren‚Äôt wrong. But Calley and Casey advocating for food and pharma improvements aren‚Äôt wrong either.Whether you like the way Calley answers or not, I am just as put off by Jack in this particular interview.It came off to me as an ‚ÄúI‚Äôm an ego maniac that needs everyone to know I‚Äôm a physician and I‚Äôm jealous that you and your sister came on scene and sold more books than me this year.‚ÄùI get the connections Jack has made in Calleys history. But Calley has said upfront that he was a lobbyist. So connections to those elites isn‚Äôt a surprise. Either way, no one knows your body like you so do your own research and do not eat or take what is not beneficial to you, the individual.Sad, but until changes are employed, this is what our ‚Äúhealthcare‚Äù has become‚Ä¶\n",
      "7. Oh please CalleyI have had my naturopathic medical license since 2007. I would love to speak ‚Äúfrom my heart‚Äù and let the audience ‚Äúdecide‚Äù who is the BEST spokesperson on food and health. How hubris‚Ä¶my sister dropped out of residency and she‚Äôs the BEST spokesperson. Let‚Äôs bring Dr. Berg into the discussion as well. It would be FUN..\n",
      "8. I hope the retired Dr Jack who they brought back like you do with an  old greyhound who's past  prime is aware that NOBODY in the comments have agreed that what he says about Rockefeller and this young man holds water. If he was their doctor and he helped them, they thanked him, and that's it. So I wish this Jack would get off his pedestal of old man wants to beat up young man because he NEVER got to the position that he thought he deserved in life. Jack is straight through BS. . I'm glad he's able to do a thing or two regarding healing people but frankly considering the level of the playing field it's not much.\n",
      "9. I just tried to find Dr Jack Kruse on the web to see what school he is so not famous that he's not on the web &  obviously did not go to Harvard or Stanford or he wouldn't be dissing them. I did find a doctor Jack Kruse who solved his health problem by food and he wrote a whole book on it and created a diet to help Americans change their s*** way of eating so maybe he's jealous of that Jack Kruse  as well as this young man but I'm sick of old men being jealous and either sending out young man to die in war or beating them up in person on the web. I'm glad some of you say Dr Jack helped you but right now he is presenting himself as a Doctor Jekyll Dr Hyde.\n",
      "10. This is ridiculous.Calley & Casey Means walked out of their respective careers because their eyes were opened to the devastating effects of pharma/food on their mother.  THEIR MOTHER. If they were weaker, less intelligent, less driven individuals (clearly inherited/taught - in part- by the same mother) they wouldn't have walked away from the env'ts that caused her chronic disease and death. The timing of their efforts was perfect for gaining awareness & coincided with (likely) the biggest mass-awakening in US history. Tucker then opened the floodgates when he interviewed Calley.People who have been insiders & then had awakenings are exactly the people you should WANT on your side.  They are whistleblowers.Who makes the best doctors? The ones who've woken up to the corruption & limitations of their own training in allopathic medicine, and retrain in functional med. This is EXACTLY what Casey Means is, and Calley is her analog from the PR/marketing world.Jack's incessant unhinged & ludicrously conspiratorial attack just makes him look hysterical.Mary's rage after her treatment at the hands of Houston Methodist and the TX medical board is completely understandable, but pointing fingers at Calley because he happened to have effectively the same awakening as her but has been more successful smacks of envy.\n",
      "11. Lol... It's only 3:40 in and the Janus was called out. Only a \"review\" is being called for??? How many more discussions/ presentations/ research projects need to be produced before the line could be drawn in the sand? Fruit loops is the chosen scapegoat, apparently. I hope R.F. Kennedy, Trump and the rest of their cabinet colleague is watching this video and realising the two-face who is hanging around him. No one is saying there is no institutional pressure, but who dared to say black is black and white is white? Alternatively, what is the possibility that the Means siblings are some kind of triple agents playing the long game?Anyway, please don't forget the millions of other children and adult consumers in the rest of the world who are equally captured by the pharma industry, who are living in countries without the benefit of some health issue debates going on because the \"expert\" opinion based on imported Western science is sacrosanct.\n",
      "12. I don‚Äôt understand the sentiment‚Ä¶ Dr. J is still trying to fight yesterday‚Äòs Covid battle. I found him as insincere as many apparently think Calley was (what‚Äôs worse, a shock jock or a lobbyist?). Dr jack acted like a jerk and you don‚Äôt win someone over to your side by talking about their dead mom.\n",
      "13. I think when you put health into the conversation and your personal goal behinds paying off school debt, getting rich. You automatically go to the dark side. I think Dr. Jack and this awesome lady who is a ENT has the perfect approach\n",
      "14. The fiasco that was Covid was all part of the JQ and Jack knows this, but doesn‚Äôt come right out and say it, I admire that he sees through the smoke and mirrors.\n",
      "15. This Dr Jack may be knowledgeable but wow us he a JACK-ASS!!Is it any wonder he‚Äôs NOT asked to be on decisive policy changing committees of substance for change??Shoot- NONE OF US normal folk (majority of US or World population) knew ANY of these conspiracy things 4 years ago. For these Drs to attack Calley this way proves we will never get ANYWHERE with such an approach. I also think it‚Äôs ironic he shows up with a flipping orange Bitcoin Santa hat as a billboard  selling coins. What a shame he‚Äôs a Pompous Ass that had he played this conversation differently- he‚Äôd likely garnered a seat at the table. Jack may do well to put the chip bag down himself and read the book How To Win Friends and Influence People. Is love to see its resume of work and cohorts to start connecting dots straight to BlackRock, State Street and Vanguard. My point- WE ALL ARE IN THE MATRIX and most of us just starting to truly understand how. I‚Äôm still team Calley after this demonstration. Dr Jack Ass will NEVER make any policy board moving fwd. ü§¶üèª‚Äç‚ôÄÔ∏è\n",
      "16. This does seem adversarial, but that‚Äôs what‚Äôs missing in hard conversations now days.  Tell a PR man like Calley not to sing and dance around hard questions; they‚Äôre trained to stay in the middle of the road to not alienate people.  More people need to look into Jack, he brings some frightening  revelations about our modern medical society and what it‚Äôs based on and what it came from.  It‚Äôs unsettling in the highest sense.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def show_topic_full(topic_id, labels_dict):\n",
    "    print(f\"=== Topic {topic_id} ===\\n\")\n",
    "    \n",
    "\n",
    "    for model_name, model_labels in labels_dict.items():\n",
    "        labels = model_labels.get(topic_id, [])\n",
    "        print(f\"--- {model_name} Labels ---\")\n",
    "        print(\", \".join(labels) if labels else \"No labels\")\n",
    "        print()\n",
    "    \n",
    "\n",
    "    print(f\"--- All Texts in Topic {topic_id} ---\")\n",
    "    texts_in_topic = [text for text, t in zip(texts, topics) if t == topic_id]\n",
    "    \n",
    "    if not texts_in_topic:\n",
    "        print(\"No texts found for this topic.\")\n",
    "    else:\n",
    "        for i, text in enumerate(texts_in_topic, 1):\n",
    "            print(f\"{i}. {text}\")\n",
    "\n",
    "\n",
    "random_topic = random.choice(list(set(topics) - {-1}))\n",
    "show_topic_full(random_topic, labels_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Purity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERTopic</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KeyBERT</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Average Purity\n",
       "0  BERTopic             0.1\n",
       "1   KeyBERT             0.2\n",
       "2    Gemini             0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_cluster_purity(labels_dict):\n",
    "    purities = []\n",
    "\n",
    "    for model_name, model_labels in labels_dict.items():\n",
    "        model_purities = []\n",
    "\n",
    "        for topic_id in model_labels.keys():\n",
    "            keywords = model_labels[topic_id]\n",
    "            if len(keywords) == 0:\n",
    "                purity = 0\n",
    "            else:\n",
    "                keyword_counts = pd.Series(keywords).value_counts()\n",
    "                purity = keyword_counts.max() / len(keywords)\n",
    "            model_purities.append(purity)\n",
    "        \n",
    "        avg_purity = np.mean(model_purities)\n",
    "        purities.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Average Purity\": round(avg_purity, 3)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(purities)\n",
    "\n",
    "purity_df = compute_cluster_purity(labels_dict)\n",
    "display(purity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Label Stability ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model 1</th>\n",
       "      <th>Model 2</th>\n",
       "      <th>Stability (Avg Jaccard)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERTopic</td>\n",
       "      <td>KeyBERT</td>\n",
       "      <td>0.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BERTopic</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KeyBERT</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model 1  Model 2  Stability (Avg Jaccard)\n",
       "0  BERTopic  KeyBERT                    0.092\n",
       "1  BERTopic   Gemini                    0.009\n",
       "2   KeyBERT   Gemini                    0.045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_label_stability(labels_dict):\n",
    "    rows = []\n",
    "\n",
    "    models = list(labels_dict.keys())\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            model1 = models[i]\n",
    "            model2 = models[j]\n",
    "\n",
    "            common_topics = set(labels_dict[model1].keys()) & set(labels_dict[model2].keys())\n",
    "            if not common_topics:\n",
    "                continue\n",
    "\n",
    "            jaccard_scores = []\n",
    "\n",
    "            for topic in common_topics:\n",
    "                l1 = set(labels_dict[model1][topic])\n",
    "                l2 = set(labels_dict[model2][topic])\n",
    "                score = len(l1 & l2) / len(l1 | l2) if l1 | l2 else 0\n",
    "                jaccard_scores.append(score)\n",
    "\n",
    "            avg_stability = np.mean(jaccard_scores)\n",
    "            rows.append({\n",
    "                \"Model 1\": model1,\n",
    "                \"Model 2\": model2,\n",
    "                \"Stability (Avg Jaccard)\": round(avg_stability, 3)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# --- ◊î◊®◊¶◊î ---\n",
    "stability_df = compute_label_stability(labels_dict)\n",
    "print(\"=== Label Stability ===\")\n",
    "display(stability_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Model Ranking ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Purity</th>\n",
       "      <th>Stability</th>\n",
       "      <th>Final Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KeyBERT</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.1340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERTopic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Average Purity  Stability  Final Score\n",
       "1   KeyBERT             0.2      0.068       0.1340\n",
       "2    Gemini             0.2      0.027       0.1135\n",
       "0  BERTopic             0.1      0.050       0.0750"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def final_model_ranking(purity_df, stability_df):\n",
    "    # ◊ó◊ô◊©◊ï◊ë ◊û◊û◊ï◊¶◊¢ Stability ◊ú◊õ◊ú ◊û◊ï◊ì◊ú\n",
    "    stabilities = []\n",
    "    for model in purity_df[\"Model\"]:\n",
    "        model_stabilities = []\n",
    "        for _, row in stability_df.iterrows():\n",
    "            if row[\"Model 1\"] == model or row[\"Model 2\"] == model:\n",
    "                model_stabilities.append(row[\"Stability (Avg Jaccard)\"])\n",
    "        avg_stability = np.mean(model_stabilities) if model_stabilities else 0\n",
    "        stabilities.append(round(avg_stability, 3))\n",
    "\n",
    "    # ◊î◊ï◊°◊§◊™ ◊î◊¢◊û◊ï◊ì◊î ◊ú◊ò◊ë◊ú◊î\n",
    "    purity_df[\"Stability\"] = stabilities\n",
    "\n",
    "    # --- ◊û◊©◊ß◊ï◊ú◊ï◊™ (◊™◊ï◊õ◊ú ◊ú◊©◊†◊ï◊™ ◊ë◊î◊™◊ê◊ù ◊ú◊¶◊ï◊®◊ö) ---\n",
    "    w1 = 0.5  # Purity\n",
    "    w2 = 0.5  # Stability\n",
    "\n",
    "    # ◊ó◊ô◊©◊ï◊ë ◊î◊¶◊ô◊ï◊ü ◊î◊°◊ï◊§◊ô\n",
    "    purity_df[\"Final Score\"] = (\n",
    "        w1 * purity_df[\"Average Purity\"] +\n",
    "        w2 * purity_df[\"Stability\"]\n",
    "    )\n",
    "\n",
    "    return purity_df.sort_values(\"Final Score\", ascending=False)\n",
    "\n",
    "# --- ◊î◊®◊¶◊î ---\n",
    "ranking_df = final_model_ranking(purity_df, stability_df)\n",
    "print(\"=== Final Model Ranking ===\")\n",
    "display(ranking_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
