{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 BERTopic Labeling Comparison Framework\n",
    "\n",
    "This notebook compares different methods for generating topic labels after clustering YouTube comments using BERTopic.\n",
    "\n",
    "We will compare:\n",
    "- BERTopic built-in labeling (TF-IDF based)\n",
    "- KeyBERT keyword extraction\n",
    "- Gemini (LLM) based labeling\n",
    "- Compute pairwise Jaccard similarity\n",
    "- Visualize and interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping keras as it is not installed.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping keras-nightly as it is not installed.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping keras-preprocessing as it is not installed.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping tf-keras as it is not installed.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping tensorflow as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Collecting transformers==4.36.2\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "Collecting tokenizers==0.13.3\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Collecting sentence-transformers==2.2.2\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Cannot install tokenizers==0.13.3 and transformers==4.36.2 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting bertopic==0.15.0\n",
      "  Using cached bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
      "Collecting keybert==0.7.0\n",
      "  Using cached keybert-0.7.0.tar.gz (21 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ilai\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ilai\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ilai\\anaconda3\\lib\\site-packages (3.9.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (0.29.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (2021.8.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (4.62.3)\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of keras to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested tokenizers==0.13.3\n",
      "    transformers 4.36.2 depends on tokenizers<0.19 and >=0.14\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\ilai\\anaconda3\\lib\\site-packages (0.8.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\ilai\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\n",
      "ERROR: No matching distribution found for itertools\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip uninstall -y keras keras-nightly keras-preprocessing tf-keras tensorflow\n",
    "%pip install keras==2.11.0 tf-keras transformers==4.36.2 tokenizers==0.13.3 sentence-transformers==2.2.2 bertopic==0.15.0 keybert==0.7.0 scikit-learn pandas matplotlib\n",
    "%pip install google-generativeai itertools numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "import google.generativeai as genai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load CSV\n",
    "df = pd.read_csv(\"..\\data\\youtube_comments\\jack_vs_calley_1000.csv\") \n",
    "texts = df[\"text\"].dropna().astype(str).tolist() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: BERTopic Clustering ---\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "topic_model = BERTopic(embedding_model=embedding_model)\n",
    "topics, probs = topic_model.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyDFm56mSyyYDUAL8yeWlYJ3Rf9z_fNFU9A\")\n",
    "\n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Custom Labelers ---\n",
    "class TopicLabeler:\n",
    "    def __init__(self, texts, topics):\n",
    "        self.texts = texts\n",
    "        self.topics = topics\n",
    "\n",
    "    def label_with_bertopic(self, topic_model):\n",
    "        return {topic: [word for word, _ in topic_model.get_topic(topic)] for topic in set(self.topics) if topic != -1}\n",
    "\n",
    "    def label_with_keybert(self, embedding_model, top_n=5):\n",
    "        kw_model = KeyBERT(model=embedding_model)\n",
    "        labels = {}\n",
    "        for topic in set(self.topics):\n",
    "            if topic == -1:\n",
    "                continue\n",
    "            docs_in_topic = [text for text, t in zip(self.texts, self.topics) if t == topic]\n",
    "            keywords = kw_model.extract_keywords(\" \".join(docs_in_topic), top_n=top_n, stop_words='english')\n",
    "            labels[topic] = [kw[0] for kw in keywords]\n",
    "        return labels\n",
    "    \n",
    "    def label_with_gemini(self, model, max_words=5):\n",
    "        labels = {}\n",
    "        for topic in set(self.topics):\n",
    "            if topic == -1:\n",
    "                continue\n",
    "\n",
    "            # Collect topic texts\n",
    "            docs_in_topic = [text for text, t in zip(self.texts, self.topics) if t == topic]\n",
    "\n",
    "            # Skip small topics\n",
    "            if len(docs_in_topic) < 3:\n",
    "                continue\n",
    "\n",
    "            # Limit number of comments\n",
    "            docs_in_topic = docs_in_topic[:5]\n",
    "\n",
    "            # Limit each comment length (max 300 characters per comment)\n",
    "            docs_in_topic = [text[:300] for text in docs_in_topic]\n",
    "\n",
    "            # Prepare the prompt text\n",
    "            docs_text = \"\\n\".join(docs_in_topic)\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            You are given a group of YouTube comments that share a common topic.\n",
    "            Provide up to {max_words} keywords or short phrases that best summarize the main topic of these comments.\n",
    "            Comments:\n",
    "            {docs_text}\n",
    "            Return the keywords separated by commas only.\n",
    "            \"\"\"\n",
    "\n",
    "            chat = model.start_chat()\n",
    "            response = chat.send_message(prompt)\n",
    "            keywords = response.text.strip().split(',')\n",
    "\n",
    "            labels[topic] = [kw.strip() for kw in keywords]\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Labelers and Generate Topic Labels\n",
    "\n",
    "In this step, we will generate the actual **labels** (keywords) for each topic identified by BERTopic.\n",
    "\n",
    "Each labeler takes the set of documents (comments) in each topic and outputs a list of keywords that are supposed to best describe the topic.\n",
    "\n",
    "### The Labelers:\n",
    "\n",
    "1. **BERTopic Built-in Labeler**  \n",
    "   - This is the default labeler provided by BERTopic.\n",
    "   - It uses TF-IDF to extract the most informative words within each topic cluster.\n",
    "   - Advantage: Simple, fast, and based on word frequency.\n",
    "   - Limitation: Might prioritize frequent but less meaningful words.\n",
    "\n",
    "2. **KeyBERT-based Labeler**  \n",
    "   - Uses the `KeyBERT` library to extract keywords by calculating semantic similarity between words and the overall topic embedding.\n",
    "   - Advantage: Leverages semantic information, can extract less frequent but semantically important keywords.\n",
    "   - Limitation: May produce more specific labels that are harder to generalize.\n",
    "\n",
    "3. **Gemini-based Labeler (LLM)**  \n",
    "   - Uses Google's Gemini (Generative AI) to generate keywords by prompting an LLM directly with the topic's documents.\n",
    "   - Advantage: Capable of generating human-like and context-aware labels that may capture abstract concepts.\n",
    "   - Limitation: Computationally expensive and may be influenced by prompt design.\n",
    "\n",
    "---\n",
    "\n",
    "### What Are We Comparing?\n",
    "\n",
    "We aim to compare:\n",
    "- How similar the labels produced by each model are.\n",
    "- Whether the models consistently agree on the most important words for a topic.\n",
    "- Which method generates more meaningful, diverse, and human-friendly topic descriptions.\n",
    "\n",
    "We will do this by:\n",
    "1. Generating labels with all three models.\n",
    "2. Computing **Jaccard Similarity** between every pair of models.\n",
    "3. Visualizing and analyzing the similarity results.\n",
    "4. Manually exploring topics and their generated labels.\n",
    "\n",
    "This step is crucial to understand which labeling method is most suitable for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Labeling ---\n",
    "labeler = TopicLabeler(texts, topics)\n",
    "bertopic_labels = labeler.label_with_bertopic(topic_model)\n",
    "keybert_labels = labeler.label_with_keybert(embedding_model)\n",
    "gemini_labels = labeler.label_with_gemini(gemini_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection on Similarity Analysis\n",
    "\n",
    "While we initially focused on comparing the labelers using **Jaccard Similarity** between keyword sets, this approach alone did not provide sufficient insights into the actual performance or quality of the labelers.\n",
    "\n",
    "### Why?\n",
    "\n",
    "- Jaccard Similarity only measures **overlap** between the generated keywords, but it does not tell us:\n",
    "    - How many topics were successfully labeled.\n",
    "    - How diverse or informative the labels were.\n",
    "    - Whether the models tend to generate long or short labels.\n",
    "    - The uniqueness and variability across topics.\n",
    "\n",
    "- In our case, we observed:\n",
    "    - Very low Jaccard scores across most model pairs.\n",
    "    - Inconsistent patterns that did not lead to clear conclusions.\n",
    "    - For example, Gemini consistently showed low overlap, but this did not necessarily mean it produced poor labels.\n",
    "\n",
    "---\n",
    "\n",
    "## The Motivation for Model Metrics\n",
    "\n",
    "To overcome the limitations of relying on Jaccard Similarity alone, we introduced additional metrics via the `model_metrics()` function:\n",
    "\n",
    "### Added Metrics:\n",
    "- **Effective Coverage** — how many topics received a sufficient number of keywords.\n",
    "- **Average Label Length** — are the generated labels short and clear or long and verbose?\n",
    "- **Unique Keywords** — how many distinct keywords does the model generate across all topics?\n",
    "\n",
    "These metrics allow us to:\n",
    "1. Better characterize each labeler.\n",
    "2. Identify trends beyond simple keyword overlap.\n",
    "3. Make a more informed decision when selecting a labeler for our task.\n",
    "\n",
    "> In real-world applications, a single similarity score is rarely enough.  \n",
    "> Multiple complementary metrics are needed to properly evaluate and select models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\"BERTopic\": bertopic_labels, \"KeyBERT\": keybert_labels, \"Gemini\": gemini_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitation: The Challenge of Quantifying Labeling Quality\n",
    "\n",
    "While we computed several numerical metrics such as Jaccard Similarity, Coverage, Average Label Length, and Keyword Diversity, it is important to acknowledge that **measuring the quality of topic labels is inherently challenging**.\n",
    "\n",
    "Why?\n",
    "- These metrics capture certain aspects like overlap, variety, and quantity, but they do not fully capture:\n",
    "    - The relevance of the labels.\n",
    "    - The interpretability and usefulness of the labels for humans.\n",
    "    - The semantic adequacy of the labels.\n",
    "\n",
    "In practice, selecting the most suitable labeling approach often requires **human judgment**, as numerical metrics alone may not reflect how well the labels truly describe the topics.\n",
    "\n",
    "Therefore, this analysis should be seen as a **preliminary quantitative evaluation**, which ideally should be complemented with a **qualitative (manual) inspection** of selected topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Topic 12 ===\n",
      "\n",
      "--- BERTopic Labels ---\n",
      "cringe, like, wtypical, make, me, its, put, about, many, opinions\n",
      "\n",
      "--- KeyBERT Labels ---\n",
      "cringe, like, wtypical, laughing, porn\n",
      "\n",
      "--- Gemini Labels ---\n",
      "Likes, Cringe, Doctor, Agreement, Emoji\n",
      "\n",
      "--- All Texts in Topic 12 ---\n",
      "1. Wow, I read the Means book, Good Energy, and really thought they would help to make a difference, but I have to say Callie’s sketchy performance here makes me think again. Casey is afraid to actually take a stand, and sounds much more of a politician, than a freedom fighter. He talks over everyone, doesnt answer questions, but stops them from responding or asking him questions. Very disappointing!\n",
      "2. Callie reminds me of Jamie on Yellowstone.😅 Yes, I'm stereotyping. The hidden hand of the magician always has everyone looking at one hand while they the sleight of the other hand is at work. 🎩 🪄\n",
      "3. OH MY GOD LET CALLIE EFFING TALK!!!!!!!! I literally can't watch this because of those other 2 hot heads.  Holy hell. Sorry, danny\n",
      "4. Well, this is definitely insightful, informing and such but not in the way maybe it was meant to be taken initially, but I guess Callie just comes off as like a sniffling teenager which I have a few of who can’t finish the sentence without miss directing or saying some ridiculous why did you think I said what you think I said, kind of logic Just stupid over and over for 3 to 4 hours of him just barking at everything and then saying the same old thing, what makes you think I said what I think I might’ve said because I didn’t say what you thought I might’ve said and I don’t think you heard me in etc. etc. I mean it’s pretty boring after a while you knowmaybe this wasn’t the best platform for this kind of thing this is like sandlot high school arguments which usually turn into some kind of pushing shoving bitch slap mode later\n",
      "5. Callie obviously trained very well on talking word salad for 3 hours.....only words I hear loud and clear is politics, excuse me,  I don't want Obummer making ANY of my, family, friends and coworkers decisions on health matters. Agenda, paid op that's all he is.\n",
      "6. Wow... I've never heard of the female on this podcast, so to my surprise, i was delighted that she and dr kruse were on the \"right\" side... although I've heard callie on Tucker, i really liked what he and his sister had to say, BUT now that i know about his stance on the jabby-jab, I'm not sure about him anymore 😮\n",
      "7. Callie=gaslighter Jack=Legend.\n",
      "8. You are all more on the same side than you are willing to see. The distrust that has been bred is very real and legit. Transparency and SPONTANEOUS, emotional responses breed trust. The difference between straight shooters, and smooth-talkers is apparent here. Having been inside the power bases and having to excuse and learn how to communicate in their ways, compromises ones ethics, and ability to be transparent.. I know this because I lived in and spied on them also, while having serious concerns about the longterm damage, agendas, and willful ignorance  that they swim in, and demand anyone inside to play along with.Callie may not understand  how being inside those worlds has changed  him into soundy cagey, detached, and good at spinning and dodging...at times  condescending. But it has.While the Dr is directly confrontive, and harsh, especially  around the death of Callies mother, he is justifiably suspicious  of everything,  because he's lived enough, and experienced  enough, and has special knowledge  that is being hijacked and hidden.All of you have done a good job trying to argue out differences, but the two blind spots are fircing you to go round and round.One side is  being cagey and comfortable  with that \"spin\" way of sharing who he is, (even if it's to use the corrupt system in order to be allowed to continue  being listened to), who maybe out of lack of life experience,  is unwittingly connected to darker people, and dismissing any concern about those  And the other,  trying to connect dots that may not apply here..may not be deliberate  agendas by anyone to subvert transparency,..because that IS going on in nefarious ways by powerful people,  and that IS a conscious goal by people  who have done it, are doing it, and hide it.And he is doing it accusingly and  in an intimidating way.I do think that after watching for an hour,  I am bothered by the way Callie is so controlled, and presents everything  after filtering through  what he wants us to hear.And that it does breed a feeling of truth-\"lite\" and less transparency...and misdirection.I love that he's gone on record and criticized pharma and food, and is trying to stop them from shortening more lives,  and harming the quality of life for us.That's brave.And definitely  feel the same for both drs here, about scathing criticisms of the cover up and harms by the medical industries and government. I trust them more. Not because they are drs. And not because they don't make excuses for corrupt  people  and institutions.It's because they don't seem to have any hidden subtext, or spin to anything  they say. They don't measure and filter,  or distract away from the direct points  or questions like Callie does.When you're seeing and responsible for saving lives, and you're old enough to experience  the gravity of that..and living thru  times where you were betrayed and shocked that your government  would lie and do harm...you have no time for spin, or lack of spontaneous  emotional  answers that get to the whole truth, fast.For them all to work together against the same enemies,  these issues will need to be worked thru, if they can be.I hope so.Because we NEED cooperation,  and they mostly have the same (difficult) goals that we HAVE to  work together on, and get action on, to save lives.\n",
      "9. Callie is condescending and that's disgusting behavior with this level of education!\n",
      "10. “My story is” Callie is a story teller.\n",
      "11. Bravo Jack! Callie is very evasive and misleading… but “all from the heart!”\n",
      "12. “My story” “from the heart”. Ugh so many manipulative platitudes. Couldnt be more phoney.\n",
      "13. \"Uncle Jack\" is a retarded ape, who ate way too much acid as a kid.  i can't believe Callie sat through 3 hours of this...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def show_topic_full(topic_id, labels_dict):\n",
    "    print(f\"=== Topic {topic_id} ===\\n\")\n",
    "    \n",
    "\n",
    "    for model_name, model_labels in labels_dict.items():\n",
    "        labels = model_labels.get(topic_id, [])\n",
    "        print(f\"--- {model_name} Labels ---\")\n",
    "        print(\", \".join(labels) if labels else \"No labels\")\n",
    "        print()\n",
    "    \n",
    "\n",
    "    print(f\"--- All Texts in Topic {topic_id} ---\")\n",
    "    texts_in_topic = [text for text, t in zip(texts, topics) if t == topic_id]\n",
    "    \n",
    "    if not texts_in_topic:\n",
    "        print(\"No texts found for this topic.\")\n",
    "    else:\n",
    "        for i, text in enumerate(texts_in_topic, 1):\n",
    "            print(f\"{i}. {text}\")\n",
    "\n",
    "\n",
    "random_topic = random.choice(list(set(topics) - {-1}))\n",
    "show_topic_full(random_topic, labels_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Evaluating Topic 0...\n",
      "⏳ Evaluating Topic 1...\n",
      "⏳ Evaluating Topic 2...\n",
      "⏳ Evaluating Topic 3...\n",
      "⏳ Evaluating Topic 4...\n",
      "⏳ Evaluating Topic 5...\n",
      "⏳ Evaluating Topic 6...\n",
      "⏳ Evaluating Topic 7...\n",
      "⏳ Evaluating Topic 8...\n",
      "⏳ Evaluating Topic 9...\n",
      "⏳ Evaluating Topic 10...\n",
      "⏳ Evaluating Topic 11...\n",
      "⏳ Evaluating Topic 12...\n",
      "⏳ Evaluating Topic 13...\n",
      "⏳ Evaluating Topic 14...\n",
      "⏳ Evaluating Topic 15...\n",
      "\n",
      "=== Average Scores ===\n",
      "BERTopic: 29.38/100\n",
      "KeyBERT: 46.88/100\n",
      "Gemini: 64.69/100\n"
     ]
    }
   ],
   "source": [
    "def evaluate_all_topics_with_gemini(labels_dict, model):\n",
    "    results = {name: [] for name in labels_dict.keys()}\n",
    "\n",
    "    all_topics = list(set(topics) - {-1})\n",
    "    \n",
    "    for topic_id in all_topics:\n",
    "        print(f\"⏳ Evaluating Topic {topic_id}...\")\n",
    "\n",
    "        prompt = f\"Evaluate the labeling quality for Topic {topic_id}.\\n\"\n",
    "        prompt += \"For each model, here are the labels it generated:\\n\\n\"\n",
    "\n",
    "        for model_name, model_labels in labels_dict.items():\n",
    "            labels = model_labels.get(topic_id, [])\n",
    "            prompt += f\"--- {model_name} Labels ---\\n\"\n",
    "            prompt += \", \".join(labels) if labels else \"No labels\"\n",
    "            prompt += \"\\n\\n\"\n",
    "\n",
    "        prompt += \"--- Example Texts in this Topic ---\\n\"\n",
    "        texts_in_topic = [text for text, t in zip(texts, topics) if t == topic_id]\n",
    "        \n",
    "        for i, text in enumerate(texts_in_topic, 1):\n",
    "            prompt += f\"{i}. {text}\\n\"\n",
    "\n",
    "        prompt += (\"\\n\\nPlease rate each model from 1 to 100, based on how well the labels describe the topic and make sense.\\n\"\n",
    "                   \"Imagine you are a professional linguist and data scientist who was not involved in generating these labels.\\n\"\n",
    "                   \"Your task is to objectively evaluate each set of labels without any consideration of their source. Focus only on clarity, relevance, and how well the labels describe the topic's content.\\n\"\n",
    "                   \"Give only numeric ratings like this:\\n\"\n",
    "                   \"- BERTopic: <score>\\n\"\n",
    "                   \"- KeyBERT: <score>\\n\"\n",
    "                   \"- Gemini: <score>\\n\")\n",
    "\n",
    "        # פנייה ל-Gemini\n",
    "        chat = model.start_chat()\n",
    "        response = chat.send_message(prompt)\n",
    "\n",
    "        # הוצאת הציונים מהתשובה\n",
    "        for model_name in results.keys():\n",
    "            try:\n",
    "                line = [line for line in response.text.splitlines() if model_name in line][0]\n",
    "                score = int(''.join(filter(str.isdigit, line)))\n",
    "                results[model_name].append(score)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to extract score for {model_name} in Topic {topic_id}: {e}\")\n",
    "\n",
    "    # חישוב ממוצעים\n",
    "    avg_scores = {model: round(np.mean(scores), 2) if scores else 0 for model, scores in results.items()}\n",
    "\n",
    "    # הצגה מסודרת\n",
    "    print(\"\\n=== Average Scores ===\")\n",
    "    for model, score in avg_scores.items():\n",
    "        print(f\"{model}: {score}/100\")\n",
    "\n",
    "    return avg_scores\n",
    "\n",
    "# --- הרצה ---\n",
    "avg_scores = evaluate_all_topics_with_gemini(labels_dict, gemini_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Raw Gemini Response ===\n",
      "BERTopic: 29.38/100\n",
      "Explanation: Likely struggled with creating clear and interpretable labels, or the labels generated lacked relevance to the overall topic or were too broad/narrow.\n",
      "\n",
      "KeyBERT: 46.88/100\n",
      "Explanation: Probably produced labels that were somewhat relevant but possibly lacked the nuance or specificity needed for high-quality label creation. KeyBERT may also have produced labels that were hard to understand or were not cohesive.\n",
      "\n",
      "Gemini: 64.69/100\n",
      "Explanation: Generally performed well, potentially creating labels that were clear, relevant, and somewhat interpretable, demonstrating a good understanding of the context and semantic relationships within the data. Could be improved with more precise or insightful labeling.\n",
      "\n",
      "=== Explanations ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERTopic</td>\n",
       "      <td>29.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KeyBERT</td>\n",
       "      <td>46.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>64.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Average Score\n",
       "0  BERTopic          29.38\n",
       "1   KeyBERT          46.88\n",
       "2    Gemini          64.69"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def explain_scores_with_gemini(avg_scores, model):\n",
    "    # Prompt\n",
    "    prompt = \"You are an objective evaluator.\\n\"\n",
    "    prompt += \"Please explain briefly for each model why it might have achieved its respective average score.\\n\"\n",
    "    prompt += \"Focus on label quality (clarity, relevance, interpretability).\\n\"\n",
    "    prompt += \"Respond with lines in the following format:\\n\"\n",
    "    prompt += \"<Model>: <score>/100\\nExplanation: <short explanation>\\n\\n\"\n",
    "\n",
    "    for model_name, score in avg_scores.items():\n",
    "        prompt += f\"{model_name}: {score}/100\\n\"\n",
    "\n",
    "    # Gemini call\n",
    "    chat = model.start_chat()\n",
    "    response = chat.send_message(prompt)\n",
    "\n",
    "    print(\"=== Raw Gemini Response ===\")\n",
    "    print(response.text)\n",
    "\n",
    "    # More robust extraction\n",
    "    explanation_dict = {}\n",
    "    lines = response.text.splitlines()\n",
    "    current_model = None\n",
    "\n",
    "    for line in lines:\n",
    "        for model_name in avg_scores.keys():\n",
    "            if model_name in line and ':' in line:\n",
    "                current_model = model_name\n",
    "                break\n",
    "\n",
    "        if current_model and \"Explanation\" in line:\n",
    "            explanation = line.split(\"Explanation:\")[-1].strip()\n",
    "            explanation_dict[current_model] = explanation\n",
    "            current_model = None\n",
    "\n",
    "    # fallback\n",
    "    for model in avg_scores.keys():\n",
    "        if model not in explanation_dict:\n",
    "            explanation_dict[model] = \"Missing\"\n",
    "\n",
    "    # DF\n",
    "    df = pd.DataFrame([\n",
    "        {\"Model\": model, \"Average Score\": avg_scores[model]}\n",
    "        for model in avg_scores.keys()\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Usage ---\n",
    "explanation_df = explain_scores_with_gemini(avg_scores, gemini_model)\n",
    "\n",
    "print(\"=== Explanations ===\")\n",
    "display(explanation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Purity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERTopic</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KeyBERT</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Average Purity\n",
       "0  BERTopic             0.1\n",
       "1   KeyBERT             0.2\n",
       "2    Gemini             0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_cluster_purity(labels_dict):\n",
    "    purities = []\n",
    "\n",
    "    for model_name, model_labels in labels_dict.items():\n",
    "        model_purities = []\n",
    "\n",
    "        for topic_id in model_labels.keys():\n",
    "            keywords = model_labels[topic_id]\n",
    "            if len(keywords) == 0:\n",
    "                purity = 0\n",
    "            else:\n",
    "                keyword_counts = pd.Series(keywords).value_counts()\n",
    "                purity = keyword_counts.max() / len(keywords)\n",
    "            model_purities.append(purity)\n",
    "        \n",
    "        avg_purity = np.mean(model_purities)\n",
    "        purities.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Average Purity\": round(avg_purity, 3)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(purities)\n",
    "\n",
    "purity_df = compute_cluster_purity(labels_dict)\n",
    "display(purity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Label Stability ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model 1</th>\n",
       "      <th>Model 2</th>\n",
       "      <th>Stability (Avg Jaccard)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERTopic</td>\n",
       "      <td>KeyBERT</td>\n",
       "      <td>0.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BERTopic</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KeyBERT</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model 1  Model 2  Stability (Avg Jaccard)\n",
       "0  BERTopic  KeyBERT                    0.092\n",
       "1  BERTopic   Gemini                    0.009\n",
       "2   KeyBERT   Gemini                    0.045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_label_stability(labels_dict):\n",
    "    rows = []\n",
    "\n",
    "    models = list(labels_dict.keys())\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            model1 = models[i]\n",
    "            model2 = models[j]\n",
    "\n",
    "            common_topics = set(labels_dict[model1].keys()) & set(labels_dict[model2].keys())\n",
    "            if not common_topics:\n",
    "                continue\n",
    "\n",
    "            jaccard_scores = []\n",
    "\n",
    "            for topic in common_topics:\n",
    "                l1 = set(labels_dict[model1][topic])\n",
    "                l2 = set(labels_dict[model2][topic])\n",
    "                score = len(l1 & l2) / len(l1 | l2) if l1 | l2 else 0\n",
    "                jaccard_scores.append(score)\n",
    "\n",
    "            avg_stability = np.mean(jaccard_scores)\n",
    "            rows.append({\n",
    "                \"Model 1\": model1,\n",
    "                \"Model 2\": model2,\n",
    "                \"Stability (Avg Jaccard)\": round(avg_stability, 3)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# --- הרצה ---\n",
    "stability_df = compute_label_stability(labels_dict)\n",
    "print(\"=== Label Stability ===\")\n",
    "display(stability_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Model Ranking (with Gemini normalized properly) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Purity</th>\n",
       "      <th>Stability</th>\n",
       "      <th>Gemini Normalized</th>\n",
       "      <th>Final Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.6469</td>\n",
       "      <td>0.28217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KeyBERT</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.4688</td>\n",
       "      <td>0.24104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERTopic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.2938</td>\n",
       "      <td>0.14314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Average Purity  Stability  Gemini Normalized  Final Score\n",
       "2    Gemini             0.2      0.027             0.6469      0.28217\n",
       "1   KeyBERT             0.2      0.068             0.4688      0.24104\n",
       "0  BERTopic             0.1      0.050             0.2938      0.14314"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def final_model_ranking(purity_df, stability_df, gemini_explanation_df):\n",
    "    # Compute average Stability per model\n",
    "    stabilities = []\n",
    "    for model in purity_df[\"Model\"]:\n",
    "        model_stabilities = []\n",
    "        for _, row in stability_df.iterrows():\n",
    "            if row[\"Model 1\"] == model or row[\"Model 2\"] == model:\n",
    "                model_stabilities.append(row[\"Stability (Avg Jaccard)\"])\n",
    "        avg_stability = np.mean(model_stabilities) if model_stabilities else 0\n",
    "        stabilities.append(round(avg_stability, 3))\n",
    "\n",
    "    # Add Stability column\n",
    "    purity_df[\"Stability\"] = stabilities\n",
    "\n",
    "    # Merge with Gemini Scores\n",
    "    merged_df = pd.merge(purity_df, gemini_explanation_df, on=\"Model\")\n",
    "\n",
    "    # --- Weights ---\n",
    "    w1 = 0.4  # Purity\n",
    "    w2 = 0.3  # Stability\n",
    "    w3 = 0.3  # Gemini Score\n",
    "\n",
    "    # Normalize Gemini scores\n",
    "    merged_df[\"Gemini Normalized\"] = merged_df[\"Average Score\"] / 100\n",
    "\n",
    "    # Compute Final Score\n",
    "    merged_df[\"Final Score\"] = (\n",
    "        w1 * merged_df[\"Average Purity\"] +\n",
    "        w2 * merged_df[\"Stability\"] +\n",
    "        w3 * merged_df[\"Gemini Normalized\"]\n",
    "    )\n",
    "\n",
    "    # --- Remove Average Score ---\n",
    "    merged_df = merged_df.drop(columns=[\"Average Score\"])\n",
    "\n",
    "    # --- Reorder Columns ---\n",
    "    columns_order = [\"Model\", \"Average Purity\", \"Stability\", \"Gemini Normalized\", \"Final Score\"]\n",
    "    merged_df = merged_df[columns_order]\n",
    "\n",
    "    # --- Display ---\n",
    "    merged_df = merged_df.sort_values(\"Final Score\", ascending=False)\n",
    "    print(\"=== Final Model Ranking (with Gemini normalized properly) ===\")\n",
    "    display(merged_df)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "# --- הרצה ---\n",
    "ranking_df = final_model_ranking(purity_df, stability_df, explanation_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
