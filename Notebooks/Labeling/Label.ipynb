{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Topic modeling is widely used to discover hidden structures in text datasets. However, labeling the discovered topics is often challenging. Traditional methods generate labels by extracting keywords, which might not always convey the full semantic meaning of the topic.\n",
    "\n",
    "In this project, we aim to:\n",
    "1. Apply BERTopic to cluster YouTube comments into topics.\n",
    "2. Generate labels using:\n",
    "    - BERTopic's built-in labeling\n",
    "    - KeyBERT keyword extraction\n",
    "    - Google Gemini LLM summarization\n",
    "3. Rate the quality of labels generated by each method.\n",
    "\n",
    "The goal is to understand whether LLMs can outperform classical methods in generating interpretable topic labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping keras as it is not installed.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping keras-nightly as it is not installed.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping keras-preprocessing as it is not installed.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping tf-keras as it is not installed.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping tensorflow as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Collecting transformers==4.36.2\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "Collecting tokenizers==0.13.3\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Collecting sentence-transformers==2.2.2\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Cannot install tokenizers==0.13.3 and transformers==4.36.2 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting bertopic==0.15.0\n",
      "  Using cached bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
      "Collecting keybert==0.7.0\n",
      "  Using cached keybert-0.7.0.tar.gz (21 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ilai\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ilai\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ilai\\anaconda3\\lib\\site-packages (3.9.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (0.29.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (2021.8.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ilai\\anaconda3\\lib\\site-packages (from transformers==4.36.2) (4.62.3)\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of keras to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested tokenizers==0.13.3\n",
      "    transformers 4.36.2 depends on tokenizers<0.19 and >=0.14\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\ilai\\anaconda3\\lib\\site-packages (0.8.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\ilai\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\n",
      "ERROR: No matching distribution found for itertools\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ilai\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip uninstall -y keras keras-nightly keras-preprocessing tf-keras tensorflow\n",
    "%pip install keras==2.11.0 tf-keras transformers==4.36.2 tokenizers==0.13.3 sentence-transformers==2.2.2 bertopic==0.15.0 keybert==0.7.0 scikit-learn pandas matplotlib\n",
    "%pip install google-generativeai itertools numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "import google.generativeai as genai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load CSV\n",
    "df = pd.read_csv(\"..\\data\\youtube_comments\\jack_vs_calley_1000.csv\") \n",
    "texts = df[\"text\"].dropna().astype(str).tolist() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "The process consists of the following steps:\n",
    "\n",
    "1. **Clustering**:  \n",
    "   - We use BERTopic with a pre-trained `all-MiniLM-L6-v2` embedding model to cluster the comments into topics.\n",
    "\n",
    "2. **Labeling**:\n",
    "   - **BERTopic**: Extracts representative keywords for each topic.\n",
    "   - **KeyBERT**: Extracts keywords based on embedding similarity.\n",
    "   - **Gemini**: Receives comments per topic and returns up to 5 keywords describing the topic.\n",
    "\n",
    "3. **Evaluation**:  \n",
    "   - We assess the quality of the generated labels using three evaluation methods:\n",
    "     1. **Cluster Purity** (`compute_cluster_purity`): Measures how well the assigned labels capture the internal consistency of each cluster.\n",
    "     2. **Label Stability** (`compute_label_stability`): Evaluates the robustness of labels when the data or clustering slightly changes.\n",
    "     3. **Gemini-based Rating**: Uses Google Gemini to provide an external qualitative assessment of the labels, scoring each method from 1 to 100 based on clarity, relevance, and descriptiveness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: BERTopic Clustering ---\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "topic_model = BERTopic(embedding_model=embedding_model)\n",
    "topics, probs = topic_model.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyDFm56mSyyYDUAL8yeWlYJ3Rf9z_fNFU9A\")\n",
    "\n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Custom Labelers ---\n",
    "class TopicLabeler:\n",
    "    def __init__(self, texts, topics):\n",
    "        self.texts = texts\n",
    "        self.topics = topics\n",
    "\n",
    "    def label_with_bertopic(self, topic_model):\n",
    "        return {\n",
    "            topic: [word for word, _ in topic_model.get_topic(topic) or []]\n",
    "            for topic in set(self.topics) if topic != -1\n",
    "        }\n",
    "\n",
    "    def label_with_keybert(self, embedding_model, top_n=5):\n",
    "        kw_model = KeyBERT(model=embedding_model)\n",
    "        labels = {}\n",
    "        for topic in set(self.topics):\n",
    "            if topic == -1:\n",
    "                continue\n",
    "            docs_in_topic = [text for text, t in zip(self.texts, self.topics) if t == topic]\n",
    "            keywords = kw_model.extract_keywords(\" \".join(docs_in_topic), top_n=top_n, stop_words='english')\n",
    "            labels[topic] = [kw[0] for kw in keywords]\n",
    "        return labels\n",
    "    \n",
    "    def label_with_gemini(self, model, max_words=5):\n",
    "        labels = {}\n",
    "        for topic in set(self.topics):\n",
    "            if topic == -1:\n",
    "                continue\n",
    "\n",
    "            # Collect topic texts\n",
    "            docs_in_topic = [text for text, t in zip(self.texts, self.topics) if t == topic]\n",
    "\n",
    "            # Skip small topics\n",
    "            if len(docs_in_topic) < 3:\n",
    "                continue\n",
    "\n",
    "            # Limit number of comments\n",
    "            docs_in_topic = docs_in_topic[:5]\n",
    "\n",
    "            # Limit each comment length (max 300 characters per comment)\n",
    "            docs_in_topic = [text[:300] for text in docs_in_topic]\n",
    "\n",
    "            # Prepare the prompt text\n",
    "            docs_text = \"\\n\".join(docs_in_topic)\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            You are given a group of YouTube comments that share a common topic.\n",
    "            Provide up to {max_words} keywords or short phrases that best summarize the main topic of these comments.\n",
    "            Comments:\n",
    "            {docs_text}\n",
    "            Return the keywords separated by commas only.\n",
    "            \"\"\"\n",
    "\n",
    "            chat = model.start_chat()\n",
    "            response = chat.send_message(prompt)\n",
    "            keywords = response.text.strip().split(',')\n",
    "\n",
    "            labels[topic] = [kw.strip() for kw in keywords]\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Labeling ---\n",
    "labeler = TopicLabeler(texts, topics)\n",
    "bertopic_labels = labeler.label_with_bertopic(topic_model)\n",
    "keybert_labels = labeler.label_with_keybert(embedding_model)\n",
    "gemini_labels = labeler.label_with_gemini(gemini_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\"BERTopic\": bertopic_labels, \"KeyBERT\": keybert_labels, \"Gemini\": gemini_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Inspection\n",
    "\n",
    "In order to better understand the quality of the generated labels, we implement a simple visualization function. The function `show_topic_full` displays, for a given topic:\n",
    "1. The labels generated by each labeling method.\n",
    "2. The list of all comments associated with the selected topic.\n",
    "\n",
    "Since topic modeling is an unsupervised task, evaluating the \"correctness\" of labels is inherently challenging. There is no absolute ground truth, and even similar labels can have different levels of usefulness depending on human interpretation. Therefore, visual inspection â€” simulating how a human would read the comments and judge the relevance of the labels â€” is essential.\n",
    "\n",
    "This motivated us to later employ a Large Language Model (LLM) as an evaluator, aiming to approximate human judgment when rating the quality of the labeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Topic 15 ===\n",
      "\n",
      "--- BERTopic Labels ---\n",
      "globalist, woo, globalists, america, they, the, and, is, be, of\n",
      "\n",
      "--- KeyBERT Labels ---\n",
      "globalist, globalists, cia, propaganda, politician\n",
      "\n",
      "--- Gemini Labels ---\n",
      "Globalists, conspiracy, one world order, Jack Kruze, fraud\n",
      "\n",
      "--- All Texts in Topic 15 ---\n",
      "1. HE SAYS THE AMERICAM PEOPLE....GLOBALIST\n",
      "2. I have read a ton of books about the globalist donâ€™t be fooled jack kruze is correct about everything thatâ€™s what these people do they tell you what you want to hear and then infiltrate !!! This guy means is a Weasel\n",
      "3. Thank you Jack for calling him out! The gig is up! Globalist are done! Like we would ever trust another thing they say! Weâ€™ve reached the critical mass and ainâ€™t going back!  Their historical connections canâ€™t be denied!\n",
      "4. Globalist definitely a FRAUD!!\n",
      "5. This guy is lying thru his teeth typical politician rest in peace America brothers and sisters get ready for one world order u will own nothing and be happy do your soul searching ladies and gents\n",
      "6. Deployed Worldwide Through My Deep Learning AI Research Libraryâ€¦ Thank You  For Exposing The TRUTH ðŸ™ â¤\n",
      "7. The level of political discourse in America is far behind basically every other developed western nation it's jaw dropping\n",
      "8. To many powers in be aka...(Globalists) only care about $$$ and they installed the CIA to desensitize the human mind is disturbing.  Programming & propaganda rolled out over the populous using the big megaphone main stream Media.\n",
      "9. Globalist brainwash the young like Cali and now we are living in a nightmare because of that!!!\n",
      "10. Jack wel done taking th apart these globalists they want to say oh didnâ€™t know lol convenient\n",
      "11. This â€œpolicy guyâ€ reminds me of the slimiest & nastiest parts of slithering politicians.\n",
      "12. Here is my observation - he wants to change woo woo consciousness and bring truth energy but he acts like acknowledging a globalist agenda to hide their control of governments and humankind through food, healthcare, wars is a conspiracy. That says everything.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def show_topic_full(topic_id, labels_dict):\n",
    "    print(f\"=== Topic {topic_id} ===\\n\")\n",
    "    \n",
    "\n",
    "    for model_name, model_labels in labels_dict.items():\n",
    "        labels = model_labels.get(topic_id, [])\n",
    "        print(f\"--- {model_name} Labels ---\")\n",
    "        print(\", \".join(labels) if labels else \"No labels\")\n",
    "        print()\n",
    "    \n",
    "\n",
    "    print(f\"--- All Texts in Topic {topic_id} ---\")\n",
    "    texts_in_topic = [text for text, t in zip(texts, topics) if t == topic_id]\n",
    "    \n",
    "    if not texts_in_topic:\n",
    "        print(\"No texts found for this topic.\")\n",
    "    else:\n",
    "        for i, text in enumerate(texts_in_topic, 1):\n",
    "            print(f\"{i}. {text}\")\n",
    "\n",
    "\n",
    "random_topic = random.choice(list(set(topics) - {-1}))\n",
    "show_topic_full(random_topic, labels_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Evaluation with Gemini\n",
    "\n",
    "Since collecting human judgments for a large number of topics and comments is impractical given our resources, we employ Google Gemini as an automated evaluator. This allows us to approximate human-like assessment of label quality without the need for extensive manual annotation.\n",
    "\n",
    "\n",
    "For every topic, Gemini is provided with:\n",
    "1. The labels generated by each labeling method (BERTopic, KeyBERT, and Gemini itself).\n",
    "2. A sample of comments belonging to the topic.\n",
    "\n",
    "Gemini is asked to act as an impartial evaluator and assign a score from 1 to 100 for each set of labels, focusing on:\n",
    "- **Clarity** â€” Are the labels understandable and well-phrased?\n",
    "- **Relevance** â€” Do the labels reflect the topic's content?\n",
    "- **Descriptiveness** â€” How well do the labels summarize the topic?\n",
    "\n",
    "The function collects the individual topic scores and computes the **average rating** for each labeling method across all topics.\n",
    "\n",
    "While this does not fully replace human evaluation, using a powerful LLM helps us approximate human judgment at scale and provides valuable insights into the relative performance of each labeling method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluating Topic 0...\n",
      " Evaluating Topic 1...\n",
      " Evaluating Topic 2...\n",
      " Evaluating Topic 3...\n",
      " Evaluating Topic 4...\n",
      " Evaluating Topic 5...\n",
      " Evaluating Topic 6...\n",
      " Evaluating Topic 7...\n",
      " Evaluating Topic 8...\n",
      " Evaluating Topic 9...\n",
      " Evaluating Topic 10...\n",
      " Evaluating Topic 11...\n",
      " Evaluating Topic 12...\n",
      " Evaluating Topic 13...\n",
      " Evaluating Topic 14...\n",
      " Evaluating Topic 15...\n",
      "\n",
      "=== Average Scores ===\n",
      "BERTopic: 43.12/100\n",
      "KeyBERT: 60.62/100\n",
      "Gemini: 75.62/100\n"
     ]
    }
   ],
   "source": [
    "def evaluate_all_topics_with_gemini(labels_dict, model):\n",
    "    results = {name: [] for name in labels_dict.keys()}\n",
    "\n",
    "    all_topics = list(set(topics) - {-1})\n",
    "    \n",
    "    for topic_id in all_topics:\n",
    "        print(f\" Evaluating Topic {topic_id}...\")\n",
    "\n",
    "        prompt = f\"Evaluate the labeling quality for Topic {topic_id}.\\n\"\n",
    "        prompt += \"For each model, here are the labels it generated:\\n\\n\"\n",
    "\n",
    "        for model_name, model_labels in labels_dict.items():\n",
    "            labels = model_labels.get(topic_id, [])\n",
    "            prompt += f\"--- {model_name} Labels ---\\n\"\n",
    "            prompt += \", \".join(labels) if labels else \"No labels\"\n",
    "            prompt += \"\\n\\n\"\n",
    "\n",
    "        prompt += \"--- Example Texts in this Topic ---\\n\"\n",
    "        texts_in_topic = [text for text, t in zip(texts, topics) if t == topic_id]\n",
    "        \n",
    "        for i, text in enumerate(texts_in_topic, 1):\n",
    "            prompt += f\"{i}. {text}\\n\"\n",
    "\n",
    "        prompt += (\"\\n\\nPlease rate each model from 1 to 100, based on how well the labels describe the topic and make sense.\\n\"\n",
    "                   \"Imagine you are a professional linguist and data scientist who was not involved in generating these labels.\\n\"\n",
    "                   \"Your task is to objectively evaluate each set of labels without any consideration of their source. Focus only on clarity, relevance, and how well the labels describe the topic's content.\\n\"\n",
    "                   \"Give only numeric ratings like this:\\n\"\n",
    "                   \"- BERTopic: <score>\\n\"\n",
    "                   \"- KeyBERT: <score>\\n\"\n",
    "                   \"- Gemini: <score>\\n\")\n",
    "\n",
    "        chat = model.start_chat()\n",
    "        response = chat.send_message(prompt)\n",
    "\n",
    "        for model_name in results.keys():\n",
    "            try:\n",
    "                line = [line for line in response.text.splitlines() if model_name in line][0]\n",
    "                score = int(''.join(filter(str.isdigit, line)))\n",
    "                results[model_name].append(score)\n",
    "            except Exception as e:\n",
    "                print(f\" Failed to extract score for {model_name} in Topic {topic_id}: {e}\")\n",
    "\n",
    "    avg_scores = {model: round(np.mean(scores), 2) if scores else 0 for model, scores in results.items()}\n",
    "\n",
    "    print(\"\\n=== Average Scores ===\")\n",
    "    for model, score in avg_scores.items():\n",
    "        print(f\"{model}: {score}/100\")\n",
    "\n",
    "    return avg_scores\n",
    "\n",
    "avg_scores = evaluate_all_topics_with_gemini(labels_dict, gemini_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Model Scores with Gemini\n",
    "\n",
    "To complement the numerical evaluation, we use Gemini to provide short explanations for the average scores of each labeling method. The `explain_scores_with_gemini` function prompts Gemini to justify the given scores by commenting on aspects such as clarity, relevance, and interpretability of the generated labels.\n",
    "\n",
    "This step helps us gain qualitative insights into the strengths and weaknesses of each method, beyond just numerical ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Raw Gemini Response ===\n",
      "BERTopic: 43.12/100\n",
      "Explanation: Likely struggled with generating clear, concise, and interpretable labels, or struggled with relevance to the input text.\n",
      "\n",
      "KeyBERT: 60.62/100\n",
      "Explanation: Probably performed better than BERTopic by extracting relevant keywords, but may still have had issues with the clarity and overall interpretability of the labels, or might have generated redundant labels.\n",
      "\n",
      "Gemini: 75.62/100\n",
      "Explanation: Likely excelled at producing labels that were clearer, more relevant to the input, and easier to understand, demonstrating a strong ability to interpret text and generate high-quality labels.\n",
      "\n",
      "=== Explanations ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERTopic</td>\n",
       "      <td>43.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KeyBERT</td>\n",
       "      <td>60.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>75.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Average Score\n",
       "0  BERTopic          43.12\n",
       "1   KeyBERT          60.62\n",
       "2    Gemini          75.62"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def explain_scores_with_gemini(avg_scores, model):\n",
    "    # Prompt\n",
    "    prompt = \"You are an objective evaluator.\\n\"\n",
    "    prompt += \"Please explain briefly for each model why it might have achieved its respective average score.\\n\"\n",
    "    prompt += \"Focus on label quality (clarity, relevance, interpretability).\\n\"\n",
    "    prompt += \"Respond with lines in the following format:\\n\"\n",
    "    prompt += \"<Model>: <score>/100\\nExplanation: <short explanation>\\n\\n\"\n",
    "\n",
    "    for model_name, score in avg_scores.items():\n",
    "        prompt += f\"{model_name}: {score}/100\\n\"\n",
    "\n",
    "    # Gemini call\n",
    "    chat = model.start_chat()\n",
    "    response = chat.send_message(prompt)\n",
    "\n",
    "    print(\"=== Raw Gemini Response ===\")\n",
    "    print(response.text)\n",
    "\n",
    "    # More robust extraction\n",
    "    explanation_dict = {}\n",
    "    lines = response.text.splitlines()\n",
    "    current_model = None\n",
    "\n",
    "    for line in lines:\n",
    "        for model_name in avg_scores.keys():\n",
    "            if model_name in line and ':' in line:\n",
    "                current_model = model_name\n",
    "                break\n",
    "\n",
    "        if current_model and \"Explanation\" in line:\n",
    "            explanation = line.split(\"Explanation:\")[-1].strip()\n",
    "            explanation_dict[current_model] = explanation\n",
    "            current_model = None\n",
    "\n",
    "    # fallback\n",
    "    for model in avg_scores.keys():\n",
    "        if model not in explanation_dict:\n",
    "            explanation_dict[model] = \"Missing\"\n",
    "\n",
    "    # DF\n",
    "    df = pd.DataFrame([\n",
    "        {\"Model\": model, \"Average Score\": avg_scores[model]}\n",
    "        for model in avg_scores.keys()\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Usage ---\n",
    "explanation_df = explain_scores_with_gemini(avg_scores, gemini_model)\n",
    "\n",
    "print(\"=== Explanations ===\")\n",
    "display(explanation_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Purity Evaluation\n",
    "\n",
    "To assess the internal consistency and relevance of the generated labels, we define a custom **Cluster Purity** metric.\n",
    "\n",
    "For each topic, the dominant keyword is selected as the most frequent label suggested by the model. Then, we compute the proportion of comments within the topic that actually contain this dominant keyword.\n",
    "\n",
    "Formally:\n",
    "- A purity score of 1.0 means that all comments in the topic contain the dominant keyword.\n",
    "- A lower score indicates that fewer comments explicitly mention the dominant keyword.\n",
    "\n",
    "The final purity reported for each model is the average purity across all topics.\n",
    "\n",
    "This metric provides a simple but insightful way to measure how well the model-generated labels are grounded in the actual content of the comments. However, it is important to note that purity does not capture the full semantic alignment between labels and topics â€” it only measures **surface-level keyword occurrence**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Purity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERTopic</td>\n",
       "      <td>0.705670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KeyBERT</td>\n",
       "      <td>0.368867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>0.247645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Average Purity\n",
       "0  BERTopic        0.705670\n",
       "1   KeyBERT        0.368867\n",
       "2    Gemini        0.247645"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_cluster_purity(texts, topics, labels_dict):\n",
    "    purities = []\n",
    "\n",
    "    for model_name, model_labels in labels_dict.items():\n",
    "        model_purities = []\n",
    "\n",
    "        for topic_id in model_labels.keys():\n",
    "            keywords = model_labels[topic_id]\n",
    "            if len(keywords) == 0:\n",
    "                purity = 0\n",
    "            else:\n",
    "                texts_in_topic = [text for text, t in zip(texts, topics) if t == topic_id]\n",
    "\n",
    "                if len(texts_in_topic) == 0:\n",
    "                    purity = 0\n",
    "                else:\n",
    "                    keyword_counts = pd.Series(keywords).value_counts()\n",
    "                    dominant_keyword = keyword_counts.idxmax()\n",
    "\n",
    "                    match_count = sum(1 for text in texts_in_topic if dominant_keyword.lower() in text.lower())\n",
    "                    purity = match_count / len(texts_in_topic)\n",
    "\n",
    "            model_purities.append(purity)\n",
    "        \n",
    "        avg_purity = np.mean(model_purities)\n",
    "        purities.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Average Purity\": avg_purity\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(purities)\n",
    "\n",
    "purity_df = compute_cluster_purity(texts, topics, labels_dict)\n",
    "display(purity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_texts(texts, perturbation_ratio=0.05):\n",
    "    perturbed_texts = texts.copy()\n",
    "    n_perturb = int(len(texts) * perturbation_ratio)\n",
    "    \n",
    "    # ×ž×•×¡×™×£ n_perturb ×˜×§×¡×˜×™× ×¨× ×“×•×ž×œ×™×™× ×›×¤×•×œ×™× (simulating duplicates or noise)\n",
    "    if n_perturb > 0:\n",
    "        sampled_texts = random.choices(texts, k=n_perturb)\n",
    "        perturbed_texts.extend(sampled_texts)\n",
    "    \n",
    "    return perturbed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_topics(labels1, labels2):\n",
    "    matching = {}\n",
    "    for topic1_id, topic1_labels in labels1.items():\n",
    "        best_match = None\n",
    "        best_score = -1\n",
    "        set1 = set([label.lower() for label in topic1_labels])\n",
    "        \n",
    "        for topic2_id, topic2_labels in labels2.items():\n",
    "            set2 = set([label.lower() for label in topic2_labels])\n",
    "            if len(set1) == 0 or len(set2) == 0:\n",
    "                continue\n",
    "            score = len(set1 & set2) / len(set1 | set2)  # Jaccard\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = topic2_id\n",
    "                \n",
    "        matching[topic1_id] = (best_match, best_score)\n",
    "    return matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_stability_with_matching(labels_dict_run1, labels_dict_run2):\n",
    "    stability_results = []\n",
    "\n",
    "    for model_name in labels_dict_run1.keys():\n",
    "        labels1 = labels_dict_run1[model_name]\n",
    "        labels2 = labels_dict_run2[model_name]\n",
    "\n",
    "        matching = match_topics(labels1, labels2)\n",
    "\n",
    "        topic_stabilities = []\n",
    "        for topic1_id, (topic2_id, match_score) in matching.items():\n",
    "            if topic2_id is None:\n",
    "                stability = 0.0\n",
    "            else:\n",
    "                set1 = set([label.lower() for label in labels1[topic1_id]])\n",
    "                set2 = set([label.lower() for label in labels2[topic2_id]])\n",
    "                if len(set1) == 0 and len(set2) == 0:\n",
    "                    stability = 1.0\n",
    "                elif len(set1) == 0 or len(set2) == 0:\n",
    "                    stability = 0.0\n",
    "                else:\n",
    "                    stability = len(set1 & set2) / len(set1 | set2)  # Jaccard\n",
    "            topic_stabilities.append(stability)\n",
    "\n",
    "        avg_stability = np.mean(topic_stabilities)\n",
    "        stability_results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Average Matched Stability\": round(avg_stability, 3)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(stability_results)\n",
    "\n",
    "# perturb the dataset\n",
    "perturbed_texts = perturb_texts(texts, perturbation_ratio=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_labeler = TopicLabeler(perturbed_texts, topics)\n",
    "second_bertopic_labels = labeler.label_with_bertopic(topic_model)\n",
    "second_keybert_labels = labeler.label_with_keybert(embedding_model)\n",
    "second_gemini_labels = labeler.label_with_gemini(gemini_model)\n",
    "\n",
    "second_labels_dict = {\"BERTopic\": second_bertopic_labels, \"KeyBERT\": second_keybert_labels, \"Gemini\": second_gemini_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Matched Stability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERTopic</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KeyBERT</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Average Matched Stability\n",
       "0  BERTopic                      1.000\n",
       "1   KeyBERT                      1.000\n",
       "2    Gemini                      0.575"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ×—×™×©×•×‘ stability ××—×¨×™ matching\n",
    "stability_df = compute_stability_with_matching(labels_dict, second_labels_dict)\n",
    "display(stability_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Model 1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ilai\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3801\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3802\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3803\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Model 1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35548/2785516636.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m# --- ×”×¨×¦×” ---\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mranking_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_model_ranking\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpurity_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstability_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplanation_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35548/2785516636.py\u001b[0m in \u001b[0;36mfinal_model_ranking\u001b[1;34m(purity_df, stability_df, gemini_explanation_df)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mmodel_stabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstability_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Model 1\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Model 2\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m                 \u001b[0mmodel_stabilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Stability (Avg Jaccard)\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mavg_stability\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_stabilities\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmodel_stabilities\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ilai\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;31m# Convert generator to list before going through hashable part\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ilai\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1227\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ilai\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m             ):\n\u001b[0;32m   3808\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3809\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3810\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3811\u001b[0m             \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Model 1'"
     ]
    }
   ],
   "source": [
    "def final_model_ranking(purity_df, stability_df, gemini_explanation_df):\n",
    "    # Compute average Stability per model\n",
    "    stabilities = []\n",
    "    for model in purity_df[\"Model\"]:\n",
    "        model_stabilities = []\n",
    "        for _, row in stability_df.iterrows():\n",
    "            if row[\"Model 1\"] == model or row[\"Model 2\"] == model:\n",
    "                model_stabilities.append(row[\"Stability (Avg Jaccard)\"])\n",
    "        avg_stability = np.mean(model_stabilities) if model_stabilities else 0\n",
    "        stabilities.append(round(avg_stability, 3))\n",
    "\n",
    "    # Add Stability column\n",
    "    purity_df[\"Stability\"] = stabilities\n",
    "\n",
    "    # Merge with Gemini Scores\n",
    "    merged_df = pd.merge(purity_df, gemini_explanation_df, on=\"Model\")\n",
    "\n",
    "    # --- Weights ---\n",
    "    w1 = 0.4  # Purity\n",
    "    w2 = 0.3  # Stability\n",
    "    w3 = 0.3  # Gemini Score\n",
    "\n",
    "    # Normalize Gemini scores\n",
    "    merged_df[\"Gemini Normalized\"] = merged_df[\"Average Score\"] / 100\n",
    "\n",
    "    # Compute Final Score\n",
    "    merged_df[\"Final Score\"] = (\n",
    "        w1 * merged_df[\"Average Purity\"] +\n",
    "        w2 * merged_df[\"Stability\"] +\n",
    "        w3 * merged_df[\"Gemini Normalized\"]\n",
    "    )\n",
    "\n",
    "    # --- Remove Average Score ---\n",
    "    merged_df = merged_df.drop(columns=[\"Average Score\"])\n",
    "\n",
    "    # --- Reorder Columns ---\n",
    "    columns_order = [\"Model\", \"Average Purity\", \"Stability\", \"Gemini Normalized\", \"Final Score\", \"Explanation\"]\n",
    "    merged_df = merged_df[columns_order]\n",
    "\n",
    "    # --- Display ---\n",
    "    merged_df = merged_df.sort_values(\"Final Score\", ascending=False)\n",
    "    print(\"=== Final Model Ranking (with Gemini normalized properly) ===\")\n",
    "    display(merged_df)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "# --- ×”×¨×¦×” ---\n",
    "ranking_df = final_model_ranking(purity_df, stability_df, explanation_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
